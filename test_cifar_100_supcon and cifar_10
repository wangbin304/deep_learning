

# # # # from __future__ import print_function
# # # #
# # # # import sys
# # # # import argparse
# # # # import time
# # # # import math
# # # #
# # # # import torch
# # # # import torch.backends.cudnn as cudnn
# # # #
# # # # from main_ce import set_loader
# # # # from util import AverageMeter
# # # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # # from util import set_optimizer
# # # # from networks.resnet_big import SupConResNet, LinearClassifier
# # # #
# # # #
# # # #
# # # # import torch.nn.functional as F
# # # #
# # # # try:
# # # #     import apex
# # # #     from apex import amp, optimizers
# # # # except ImportError:
# # # #     pass
# # # #
# # # #
# # # # def parse_option():
# # # #     parser = argparse.ArgumentParser('argument for training')
# # # #
# # # #     parser.add_argument('--print_freq', type=int, default=10,
# # # #                         help='print frequency')
# # # #     parser.add_argument('--save_freq', type=int, default=50,
# # # #                         help='save frequency')
# # # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # # #                         help='batch_size')
# # # #     parser.add_argument('--num_workers', type=int, default=4,
# # # #                         help='num of workers to use')
# # # #     parser.add_argument('--epochs', type=int, default=100,
# # # #                         help='number of training epochs')
# # # #
# # # #     # optimization
# # # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # # #                         help='learning rate')
# # # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # # #                         help='where to decay lr, can be a list')
# # # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # # #                         help='decay rate for learning rate')
# # # #     parser.add_argument('--weight_decay', type=float, default=0,
# # # #                         help='weight decay')
# # # #     parser.add_argument('--momentum', type=float, default=0.9,
# # # #                         help='momentum')
# # # #
# # # #     # model dataset
# # # #     parser.add_argument('--model', type=str, default='resnet50')
# # # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # # #
# # # #     # other setting
# # # #     # parser.add_argument('--cosine', action='store_true',
# # # #     #                     help='using cosine annealing')
# # # #     parser.add_argument('--cosine', default='true',
# # # #                         help='using cosine annealing')
# # # #     parser.add_argument('--warm', action='store_true',
# # # #                         help='warm-up for large batch training')
# # # #
# # # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_210.pth',
# # # #                         help='path to pre-trained model')
# # # #
# # # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # # #     #                     help='path to pre-trained model')
# # # #
# # # #
# # # #     opt = parser.parse_args()
# # # #
# # # #     # set the path according to the environment
# # # #     opt.data_folder = './datasets/'
# # # #
# # # #     iterations = opt.lr_decay_epochs.split(',')
# # # #     opt.lr_decay_epochs = list([])
# # # #     for it in iterations:
# # # #         opt.lr_decay_epochs.append(int(it))
# # # #
# # # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # # #                opt.batch_size)
# # # #
# # # #     if opt.cosine:
# # # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # # #
# # # #     # warm-up for large-batch training,
# # # #     if opt.warm:
# # # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # # #         opt.warmup_from = 0.01
# # # #         opt.warm_epochs = 10
# # # #         if opt.cosine:
# # # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # # #         else:
# # # #             opt.warmup_to = opt.learning_rate
# # # #
# # # #     if opt.dataset == 'cifar10':
# # # #         opt.n_cls = 10
# # # #     elif opt.dataset == 'cifar100':
# # # #         opt.n_cls = 100
# # # #     else:
# # # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # # #
# # # #     return opt
# # # #
# # # #
# # # # def set_model(opt):
# # # #     model = SupConResNet(name=opt.model)
# # # #     criterion = torch.nn.CrossEntropyLoss()
# # # #
# # # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # # #
# # # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # # #     state_dict = ckpt['model']
# # # #
# # # #     if torch.cuda.is_available():
# # # #         if torch.cuda.device_count() > 1:
# # # #             model = torch.nn.DataParallel(model)
# # # #         else:
# # # #             new_state_dict = {}
# # # #             for k, v in state_dict.items():
# # # #                 k = k.replace("module.", "")
# # # #                 new_state_dict[k] = v
# # # #             state_dict = new_state_dict
# # # #         model = model.cuda()
# # # #         classifier = classifier.cuda()
# # # #         criterion = criterion.cuda()
# # # #         cudnn.benchmark = True
# # # #
# # # #         model.load_state_dict(state_dict)
# # # #
# # # #     return model, classifier, criterion
# # # #
# # # #
# # # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # # #     """one epoch training"""
# # # #     model.encoder.eval()
# # # #     classifier.train()
# # # #
# # # #     batch_time = AverageMeter()
# # # #     data_time = AverageMeter()
# # # #     losses = AverageMeter()
# # # #     top1 = AverageMeter()
# # # #
# # # #     end = time.time()
# # # #     for idx, (images, labels) in enumerate(train_loader):
# # # #         images = torch.cat([images[0], images[1]], dim=0)
# # # #
# # # #         data_time.update(time.time() - end)
# # # #
# # # #         images = images.cuda(non_blocking=True)
# # # #         labels = labels.cuda(non_blocking=True)
# # # #         bsz = labels.shape[0]
# # # #
# # # #         # warm-up learning rate
# # # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # # #
# # # #         # compute loss
# # # #         with torch.no_grad():
# # # #             features = model.encoder(images)
# # # #
# # # #         # ###
# # # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # # #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # # #         #
# # # #         #
# # # #         #
# # # #         #
# # # #         #
# # # #         #
# # # #         #
# # # #         # device = (torch.device('cuda')
# # # #         #           if features.is_cuda
# # # #         #           else torch.device('cpu'))
# # # #         #
# # # #         # batch_size = features.shape[0]/2
# # # #         # labels = labels.contiguous().view(-1, 1)
# # # #         # mask = torch.eq(labels, labels.T).float().to(device)
# # # #         #
# # # #         #
# # # #         # # contrast_count = features.shape[1]
# # # #         # # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # # #         #
# # # #         # # tile mask
# # # #         # mask = mask.repeat(2,2)
# # # #         # # mask-out self-contrast cases
# # # #         # logits_mask = torch.scatter(
# # # #         #     torch.ones_like(mask),
# # # #         #     1,
# # # #         #     torch.arange(batch_size*2).view(-1, 1).to(device).long(),
# # # #         #     0
# # # #         # )
# # # #         # mask = mask * logits_mask
# # # #         #
# # # #         #
# # # #         #
# # # #         # ### 计算同一类图片增强后差值的L2范数
# # # #         # # print(mask[0,:],mask.shape)
# # # #         # # print(mask[1,:],mask.shape)
# # # #         #
# # # #         # num = 0
# # # #         # # print(mask[0:2,:5])
# # # #         # mask = mask * logits_mask
# # # #         # index_mask = mask[:, :] == 1
# # # #         # # print(index_mask[0:2,:5])
# # # #         #
# # # #         # # sum_loss = torch.Tensor([0]).cuda()
# # # #         # # sum_loss_192 = torch.Tensor([0]).cuda()
# # # #         # sum_loss_64 = torch.Tensor([0]).cuda()
# # # #         # div=torch.Tensor([0]).cuda()
# # # #         # # for i in range(len(mask)):
# # # #         # #     for index_feature in contrast_feature[index_mask[i,:]]:
# # # #         # #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
# # # #         # #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
# # # #         # #         sum_loss+=loss_c_192+loss_s_64
# # # #         # #         num+=1
# # # #         # for i in range(len(mask)):
# # # #         #     # print("******")
# # # #         #     # print((contrast_feature[i] - contrast_feature[index_mask[i, :]])[:].shape)
# # # #         #     num += 1
# # # #         #     # sum_loss_192 += torch.sum(
# # # #         #     #     torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# # # #         #     #                    len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # # #         #     sum_loss_64 += torch.sum(
# # # #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # # #         #                        len(contrast_feature[index_mask[i, :]]))
# # # #         #
# # # #         #     c_192=F.normalize(torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]),dim=0),dim=1)
# # # #         #     s_64=F.normalize(torch.cat((contrast_feature[i:i+1,192:],contrast_feature[index_mask[i,:]][:,192:]),dim=0).repeat(1,3),dim=1)
# # # #         #
# # # #         #     # print(torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1)))
# # # #         #     div+=torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1))
# # # #         #     # div+=torch.sum(torch.norm(,dim=0))-,dim=0).repeat(1,3)))/(len(contrast_feature[index_mask[i, :]])+1)
# # # #         #     # print(div)
# # # #         #
# # # #         #     # print(torch.norm(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # # #         #     # loss_c_192=torch.sum(torch.norm(contrast_feature[i]-contrast_feature[index_mask[i,:]]))
# # # #         #     # print(loss_c_192)
# # # #         # # sum_loss = sum_loss_192 / sum_loss_64
# # # #         # # sum_loss_192/=num
# # # #         # # sum_loss_64/=-num
# # # #         # sum_loss_64=0.001*sum_loss_64
# # # #         # div/=num
# # # #         # div*=0.1
# # # #         # # print(div)
# # # #         # # print(sum_loss_64.data)
# # # #         # # sum_loss/=num
# # # #         # # print("sum_loss:", sum_loss)
# # # #
# # # #         # features_c_192, features_s_64= torch.split(contrast_feature, [192, 64], dim=1)
# # # #
# # # #
# # # #
# # # #         # ### 计算同一张图片增强后差值向量的l2范数
# # # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # # #         # loss_c_192 = torch.sum(torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1)) / len(
# # # #         #     torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1))
# # # #         # loss_s_64 = -torch.sum(torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1)) / len(
# # # #         #     torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1))
# # # #         # print(loss_c_192, loss_s_64)
# # # #
# # # #
# # # #         # ### 计算同一类图片增强后差值向量的L2范数
# # # #         #
# # # #         # contrast_feature=features
# # # #         # device = (torch.device('cuda')
# # # #         #           if features.is_cuda
# # # #         #           else torch.device('cpu'))
# # # #
# # # #         labels = labels.contiguous().view(-1, 1)
# # # #         # mask = torch.eq(labels, labels.T).float().to(device)
# # # #         # mask = mask.repeat(2, 2)
# # # #         # num = 0
# # # #         # index_mask = mask[:, :] == 1
# # # #         # sum_loss_192 = torch.Tensor([0]).cuda()
# # # #         # sum_loss_64 = torch.Tensor([0]).cuda()
# # # #         # for i in range(len(mask)):
# # # #         #     num += 1
# # # #         #     sum_loss_192 += torch.sum(
# # # #         #         torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# # # #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # # #         #     sum_loss_64 += torch.sum(
# # # #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # # #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # # #         # sum_loss_192/=num
# # # #         # sum_loss_64/=-num
# # # #         # print(sum_loss_192,sum_loss_64)
# # # #         # print(contrast_feature.shape)
# # # #         # features,features_64= torch.split(contrast_feature, [192, 64], dim=1)
# # # #         # print(features.shape)
# # # #
# # # #         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # # #
# # # #         labels = torch.cat((labels, labels), dim=0)
# # # #
# # # #
# # # #
# # # #
# # # #         output = classifier(features.detach())
# # # #         # output1 = classifier(features_s_64.detach(),64)
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #         labels=labels.squeeze(1)
# # # #         loss = criterion(output, labels)
# # # #
# # # #         # update metric
# # # #         losses.update(loss.item(), bsz)
# # # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # # #         top1.update(acc1[0], bsz)
# # # #
# # # #         # SGD
# # # #         optimizer.zero_grad()
# # # #         loss.backward()
# # # #         optimizer.step()
# # # #
# # # #         # measure elapsed time
# # # #         batch_time.update(time.time() - end)
# # # #         end = time.time()
# # # #
# # # #         # print info
# # # #         if (idx + 1) % opt.print_freq == 0:
# # # #             print('Train: [{0}][{1}/{2}]\t'
# # # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # # #                    data_time=data_time, loss=losses, top1=top1))
# # # #             sys.stdout.flush()
# # # #
# # # #     return losses.avg, top1.avg
# # # #
# # # #
# # # # def validate(val_loader, model, classifier, criterion, opt):
# # # #     """validation"""
# # # #     model.encoder.eval()
# # # #     classifier.eval()
# # # #
# # # #     batch_time = AverageMeter()
# # # #     losses = AverageMeter()
# # # #     top1 = AverageMeter()
# # # #
# # # #     with torch.no_grad():
# # # #         end = time.time()
# # # #         for idx, (images, labels) in enumerate(val_loader):
# # # #
# # # #
# # # #
# # # #             images = images.float().cuda()
# # # #             labels = labels.cuda()
# # # #             bsz = labels.shape[0]
# # # #
# # # #             # forward
# # # #             features=model.encoder(images)
# # # #             # features, features_64 = torch.split(features, [192, 64], dim=1)
# # # #
# # # #             output = classifier(features)
# # # #
# # # #             # output1=classifier(features_64,64)
# # # #
# # # #             loss = criterion(output, labels)
# # # #
# # # #             # update metric
# # # #             losses.update(loss.item(), bsz)
# # # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # # #             top1.update(acc1[0], bsz)
# # # #
# # # #             # measure elapsed time
# # # #             batch_time.update(time.time() - end)
# # # #             end = time.time()
# # # #
# # # #             if idx % opt.print_freq == 0:
# # # #                 print('Test: [{0}/{1}]\t'
# # # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # # #                        idx, len(val_loader), batch_time=batch_time,
# # # #                        loss=losses, top1=top1))
# # # #
# # # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # # #     return losses.avg, top1.avg
# # # #
# # # #
# # # # def main():
# # # #     best_acc = 0
# # # #     opt = parse_option()
# # # #
# # # #     # build data loader
# # # #     train_loader, val_loader = set_loader(opt)
# # # #
# # # #     # build model and criterion
# # # #     model, classifier, criterion = set_model(opt)
# # # #
# # # #     # build optimizer
# # # #     optimizer = set_optimizer(opt, classifier)
# # # #
# # # #     # training routine
# # # #     for epoch in range(1, opt.epochs + 1):
# # # #         adjust_learning_rate(opt, optimizer, epoch)
# # # #
# # # #         # train for one epoch
# # # #         time1 = time.time()
# # # #         loss, acc = train(train_loader, model, classifier, criterion,
# # # #                           optimizer, epoch, opt)
# # # #         time2 = time.time()
# # # #         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # # #             epoch, time2 - time1, acc))
# # # #
# # # #         # eval for one epoch
# # # #         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # # #         if val_acc > best_acc:
# # # #             best_acc = val_acc
# # # #
# # # #     print('best accuracy: {:.2f}'.format(best_acc))
# # # #
# # # #
# # # # if __name__ == '__main__':
# # # #     main()
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # # #
# # # # from __future__ import print_function
# # # #
# # # # import sys
# # # # import argparse
# # # # import time
# # # # import math
# # # #
# # # # import torch
# # # # import torch.backends.cudnn as cudnn
# # # #
# # # # from main_ce import set_loader
# # # # from util import AverageMeter
# # # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # # from util import set_optimizer
# # # # from networks.resnet_big import SupConResNet, LinearClassifier
# # # #
# # # #
# # # #
# # # # import torch.nn.functional as F
# # # #
# # # # try:
# # # #     import apex
# # # #     from apex import amp, optimizers
# # # # except ImportError:
# # # #     pass
# # # #
# # # #
# # # # def parse_option():
# # # #     parser = argparse.ArgumentParser('argument for training')
# # # #
# # # #     parser.add_argument('--print_freq', type=int, default=10,
# # # #                         help='print frequency')
# # # #     parser.add_argument('--save_freq', type=int, default=50,
# # # #                         help='save frequency')
# # # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # # #                         help='batch_size')
# # # #     parser.add_argument('--num_workers', type=int, default=4,
# # # #                         help='num of workers to use')
# # # #     parser.add_argument('--epochs', type=int, default=100,
# # # #                         help='number of training epochs')
# # # #
# # # #     # optimization
# # # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # # #                         help='learning rate')
# # # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # # #                         help='where to decay lr, can be a list')
# # # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # # #                         help='decay rate for learning rate')
# # # #     parser.add_argument('--weight_decay', type=float, default=0,
# # # #                         help='weight decay')
# # # #     parser.add_argument('--momentum', type=float, default=0.9,
# # # #                         help='momentum')
# # # #
# # # #     # model dataset
# # # #     parser.add_argument('--model', type=str, default='resnet50')
# # # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # # #
# # # #     # other setting
# # # #     # parser.add_argument('--cosine', action='store_true',
# # # #     #                     help='using cosine annealing')
# # # #     parser.add_argument('--cosine', default='true',
# # # #                         help='using cosine annealing')
# # # #     parser.add_argument('--warm', action='store_true',
# # # #                         help='warm-up for large batch training')
# # # #
# # # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# # # #                         help='path to pre-trained model')
# # # #
# # # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # # #     #                     help='path to pre-trained model')
# # # #
# # # #
# # # #     opt = parser.parse_args()
# # # #
# # # #     # set the path according to the environment
# # # #     opt.data_folder = './datasets/'
# # # #
# # # #     iterations = opt.lr_decay_epochs.split(',')
# # # #     opt.lr_decay_epochs = list([])
# # # #     for it in iterations:
# # # #         opt.lr_decay_epochs.append(int(it))
# # # #
# # # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # # #                opt.batch_size)
# # # #
# # # #     if opt.cosine:
# # # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # # #
# # # #     # warm-up for large-batch training,
# # # #     if opt.warm:
# # # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # # #         opt.warmup_from = 0.01
# # # #         opt.warm_epochs = 10
# # # #         if opt.cosine:
# # # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # # #         else:
# # # #             opt.warmup_to = opt.learning_rate
# # # #
# # # #     if opt.dataset == 'cifar10':
# # # #         opt.n_cls = 10
# # # #     elif opt.dataset == 'cifar100':
# # # #         opt.n_cls = 100
# # # #     else:
# # # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # # #
# # # #     return opt
# # # #
# # # #
# # # # def set_model(opt):
# # # #     model = SupConResNet(name=opt.model)
# # # #     criterion = torch.nn.CrossEntropyLoss()
# # # #
# # # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # # #
# # # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # # #     state_dict = ckpt['model']
# # # #
# # # #     if torch.cuda.is_available():
# # # #         if torch.cuda.device_count() > 1:
# # # #             model = torch.nn.DataParallel(model)
# # # #         else:
# # # #             new_state_dict = {}
# # # #             for k, v in state_dict.items():
# # # #                 k = k.replace("module.", "")
# # # #                 new_state_dict[k] = v
# # # #             state_dict = new_state_dict
# # # #         model = model.cuda()
# # # #         classifier = classifier.cuda()
# # # #         criterion = criterion.cuda()
# # # #         cudnn.benchmark = True
# # # #
# # # #         model.load_state_dict(state_dict)
# # # #
# # # #     return model, classifier, criterion
# # # #
# # # #
# # # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # # #     """one epoch training"""
# # # #     model.eval()
# # # #     classifier.train()
# # # #
# # # #     batch_time = AverageMeter()
# # # #     data_time = AverageMeter()
# # # #     losses = AverageMeter()
# # # #     top1 = AverageMeter()
# # # #
# # # #     end = time.time()
# # # #     for idx, (images, labels) in enumerate(train_loader):
# # # #         images = torch.cat([images[0], images[1]], dim=0)
# # # #
# # # #         data_time.update(time.time() - end)
# # # #
# # # #         images = images.cuda(non_blocking=True)
# # # #         labels = labels.cuda(non_blocking=True)
# # # #         bsz = labels.shape[0]
# # # #
# # # #         # warm-up learning rate
# # # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # # #
# # # #         # compute loss
# # # #         with torch.no_grad():
# # # #             features = model(images)
# # # #
# # # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # # #
# # # #
# # # #
# # # #         device = (torch.device('cuda')
# # # #                   if features.is_cuda
# # # #                   else torch.device('cpu'))
# # # #
# # # #
# # # #
# # # #         batch_size = features.shape[0]
# # # #
# # # #         labels = labels.contiguous().view(-1, 1)
# # # #         if labels.shape[0] != batch_size:
# # # #             raise ValueError('Num of labels does not match num of features')
# # # #         mask = torch.eq(labels, labels.T).float().to(device)
# # # #
# # # #
# # # #         contrast_count = features.shape[1]
# # # #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # # #
# # # #         # tile mask
# # # #         mask = mask.repeat(2, contrast_count)
# # # #         # mask-out self-contrast cases
# # # #         logits_mask = torch.scatter(
# # # #             torch.ones_like(mask),
# # # #             1,
# # # #             torch.arange(batch_size * 2).view(-1, 1).to(device),
# # # #             0
# # # #         )
# # # #         mask = mask * logits_mask
# # # #
# # # #         ### 计算同一类图片增强后差值的L2范数
# # # #         # print(mask[0,:],mask.shape)
# # # #         # print(mask[1,:],mask.shape)
# # # #
# # # #         num = 0
# # # #         mask = mask * logits_mask
# # # #         index_mask = mask[:, :] == 1
# # # #         sum_loss_64 = torch.Tensor([0]).cuda()
# # # #         div = torch.Tensor([0]).cuda()
# # # #         for i in range(len(mask)):
# # # #             num += 1
# # # #             sum_loss_64 += torch.sum(
# # # #                 torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # # #                                len(contrast_feature[index_mask[i, :]]))
# # # #
# # # #             c_192 = F.normalize(
# # # #                 torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]), dim=0), dim=1)
# # # #             s_64 = F.normalize(
# # # #                 torch.cat((contrast_feature[i:i + 1, 192:], contrast_feature[index_mask[i, :]][:, 192:]), dim=0).repeat(
# # # #                     1, 3), dim=1)
# # # #
# # # #             div += torch.sum(torch.norm(c_192 - s_64, dim=1)) / len(torch.norm(c_192 - s_64, dim=1))
# # # #
# # # #         sum_loss_64 = -0.001 * sum_loss_64
# # # #         div /= -num
# # # #         div *= 0.1
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #
# # # #         # ###
# # # #         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # # #         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # # #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # # #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # # #
# # # #         # labels = labels.contiguous().view(-1, 1)
# # # #
# # # #
# # # #         labels = torch.cat((labels, labels), dim=0)
# # # #
# # # #         output = classifier(features.detach())
# # # #
# # # #
# # # #         labels=labels.squeeze(1)
# # # #         loss = criterion(output, labels)
# # # #         loss=loss+div+sum_loss_64
# # # #
# # # #         # update metric
# # # #         losses.update(loss.item(), bsz)
# # # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # # #         top1.update(acc1[0], bsz)
# # # #
# # # #         # SGD
# # # #         optimizer.zero_grad()
# # # #         loss.backward()
# # # #         optimizer.step()
# # # #
# # # #         # measure elapsed time
# # # #         batch_time.update(time.time() - end)
# # # #         end = time.time()
# # # #
# # # #         # print info
# # # #         if (idx + 1) % opt.print_freq == 0:
# # # #             print('Train: [{0}][{1}/{2}]\t'
# # # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # # #                    data_time=data_time, loss=losses, top1=top1))
# # # #             sys.stdout.flush()
# # # #
# # # #     return losses.avg, top1.avg
# # # #
# # # #
# # # # def validate(val_loader, model, classifier, criterion, opt):
# # # #     """validation"""
# # # #     model.eval()
# # # #     classifier.eval()
# # # #
# # # #     batch_time = AverageMeter()
# # # #     losses = AverageMeter()
# # # #     top1 = AverageMeter()
# # # #
# # # #     with torch.no_grad():
# # # #         end = time.time()
# # # #         for idx, (images, labels) in enumerate(val_loader):
# # # #
# # # #
# # # #
# # # #             images = images.float().cuda()
# # # #             labels = labels.cuda()
# # # #             bsz = labels.shape[0]
# # # #
# # # #             # forward
# # # #             features=model(images)
# # # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # # #
# # # #             output = classifier(features)
# # # #
# # # #             # output1=classifier(features_64,64)
# # # #
# # # #             loss = criterion(output, labels)
# # # #
# # # #             # update metric
# # # #             losses.update(loss.item(), bsz)
# # # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # # #             top1.update(acc1[0], bsz)
# # # #
# # # #             # measure elapsed time
# # # #             batch_time.update(time.time() - end)
# # # #             end = time.time()
# # # #
# # # #             if idx % opt.print_freq == 0:
# # # #                 print('Test: [{0}/{1}]\t'
# # # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # # #                        idx, len(val_loader), batch_time=batch_time,
# # # #                        loss=losses, top1=top1))
# # # #
# # # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # # #     return losses.avg, top1.avg
# # # #
# # # #
# # # # def main():
# # # #     best_acc = 0
# # # #
# # # #
# # # #     for ep in range(1000,3250,250):
# # # #         opt = parse_option()
# # # #
# # # #         # build data loader
# # # #         train_loader, val_loader = set_loader(opt)
# # # #
# # # #         # build model and criterion
# # # #         model, classifier, criterion = set_model(opt)
# # # #
# # # #         # build optimizer
# # # #         optimizer = set_optimizer(opt, classifier)
# # # #
# # # #         # training routine
# # # #         for epoch in range(1, opt.epochs + 1):
# # # #             adjust_learning_rate(opt, optimizer, epoch)
# # # #
# # # #             # train for one epoch
# # # #             time1 = time.time()
# # # #             loss, acc = train(train_loader, model, classifier, criterion,
# # # #                               optimizer, epoch, opt)
# # # #             time2 = time.time()
# # # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # # #                 epoch, time2 - time1, acc))
# # # #
# # # #             # eval for one epoch
# # # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # # #             if val_acc > best_acc:
# # # #                 best_acc = val_acc
# # # #
# # # #         print('best accuracy: {:.2f}'.format(best_acc))
# # # #
# # # #
# # # # if __name__ == '__main__':
# # # #     main()
# # # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # # #
# # # #
# # # # from __future__ import print_function
# # # #
# # # # import sys
# # # # import argparse
# # # # import time
# # # # import math
# # # #
# # # # import torch
# # # # import torch.backends.cudnn as cudnn
# # # #
# # # # from main_ce import set_loader
# # # # from util import AverageMeter
# # # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # # from util import set_optimizer
# # # # from networks.resnet_big import SupConResNet, LinearClassifier
# # # #
# # # #
# # # #
# # # # import torch.nn.functional as F
# # # #
# # # # try:
# # # #     import apex
# # # #     from apex import amp, optimizers
# # # # except ImportError:
# # # #     pass
# # # #
# # # #
# # # # def parse_option():
# # # #     parser = argparse.ArgumentParser('argument for training')
# # # #
# # # #     parser.add_argument('--print_freq', type=int, default=10,
# # # #                         help='print frequency')
# # # #     parser.add_argument('--save_freq', type=int, default=50,
# # # #                         help='save frequency')
# # # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # # #                         help='batch_size')
# # # #     parser.add_argument('--num_workers', type=int, default=4,
# # # #                         help='num of workers to use')
# # # #     parser.add_argument('--epochs', type=int, default=100,
# # # #                         help='number of training epochs')
# # # #
# # # #     # optimization
# # # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # # #                         help='learning rate')
# # # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # # #                         help='where to decay lr, can be a list')
# # # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # # #                         help='decay rate for learning rate')
# # # #     parser.add_argument('--weight_decay', type=float, default=0,
# # # #                         help='weight decay')
# # # #     parser.add_argument('--momentum', type=float, default=0.9,
# # # #                         help='momentum')
# # # #
# # # #     # model dataset
# # # #     parser.add_argument('--model', type=str, default='resnet50')
# # # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # # #
# # # #     # other setting
# # # #     # parser.add_argument('--cosine', action='store_true',
# # # #     #                     help='using cosine annealing')
# # # #     parser.add_argument('--cosine', default='true',
# # # #                         help='using cosine annealing')
# # # #     parser.add_argument('--warm', action='store_true',
# # # #                         help='warm-up for large batch training')
# # # #
# # # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# # # #                         help='path to pre-trained model')
# # # #
# # # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # # #     #                     help='path to pre-trained model')
# # # #
# # # #
# # # #     opt = parser.parse_args()
# # # #
# # # #     # set the path according to the environment
# # # #     opt.data_folder = './datasets/'
# # # #
# # # #     iterations = opt.lr_decay_epochs.split(',')
# # # #     opt.lr_decay_epochs = list([])
# # # #     for it in iterations:
# # # #         opt.lr_decay_epochs.append(int(it))
# # # #
# # # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # # #                opt.batch_size)
# # # #
# # # #     if opt.cosine:
# # # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # # #
# # # #     # warm-up for large-batch training,
# # # #     if opt.warm:
# # # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # # #         opt.warmup_from = 0.01
# # # #         opt.warm_epochs = 10
# # # #         if opt.cosine:
# # # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # # #         else:
# # # #             opt.warmup_to = opt.learning_rate
# # # #
# # # #     if opt.dataset == 'cifar10':
# # # #         opt.n_cls = 10
# # # #     elif opt.dataset == 'cifar100':
# # # #         opt.n_cls = 100
# # # #     else:
# # # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # # #
# # # #     return opt
# # # #
# # # #
# # # # def set_model(opt):
# # # #     model = SupConResNet(name=opt.model)
# # # #     criterion = torch.nn.CrossEntropyLoss()
# # # #
# # # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # # #
# # # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # # #     state_dict = ckpt['model']
# # # #
# # # #     if torch.cuda.is_available():
# # # #         if torch.cuda.device_count() > 1:
# # # #             model = torch.nn.DataParallel(model)
# # # #         else:
# # # #             new_state_dict = {}
# # # #             for k, v in state_dict.items():
# # # #                 k = k.replace("module.", "")
# # # #                 new_state_dict[k] = v
# # # #             state_dict = new_state_dict
# # # #         model = model.cuda()
# # # #         classifier = classifier.cuda()
# # # #         criterion = criterion.cuda()
# # # #         cudnn.benchmark = True
# # # #
# # # #         model.load_state_dict(state_dict)
# # # #
# # # #     return model, classifier, criterion
# # # #
# # # #
# # # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # # #     """one epoch training"""
# # # #     model.eval()
# # # #     classifier.train()
# # # #
# # # #     batch_time = AverageMeter()
# # # #     data_time = AverageMeter()
# # # #     losses = AverageMeter()
# # # #     top1 = AverageMeter()
# # # #
# # # #     end = time.time()
# # # #     for idx, (images, labels) in enumerate(train_loader):
# # # #         images = torch.cat([images[0], images[1]], dim=0)
# # # #
# # # #         data_time.update(time.time() - end)
# # # #
# # # #         images = images.cuda(non_blocking=True)
# # # #         labels = labels.cuda(non_blocking=True)
# # # #         bsz = labels.shape[0]
# # # #
# # # #         # warm-up learning rate
# # # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # # #
# # # #         # compute loss
# # # #         with torch.no_grad():
# # # #             features = model(images)
# # # #
# # # #
# # # #         # ###
# # # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # # #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # # #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # # #         features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# # # #         labels = labels.contiguous().view(-1, 1)
# # # #
# # # #
# # # #         labels = torch.cat((labels, labels), dim=0)
# # # #
# # # #         output = classifier(features.detach(),1)
# # # #         output2=classifier(features_s_64.detach(),2)
# # # #
# # # #         output=output[:,9]+0.1*torch.sum(output2,dim=1).unsqueeze(1)
# # # #
# # # #         labels=labels.squeeze(1)
# # # #         loss = criterion(output, labels)
# # # #         # loss2=criterion(output2,labels)
# # # #         # loss=loss-loss2
# # # #
# # # #         # update metric
# # # #         losses.update(loss.item(), bsz)
# # # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # # #         top1.update(acc1[0], bsz)
# # # #
# # # #         # SGD
# # # #         optimizer.zero_grad()
# # # #         loss.backward()
# # # #         optimizer.step()
# # # #
# # # #         # measure elapsed time
# # # #         batch_time.update(time.time() - end)
# # # #         end = time.time()
# # # #
# # # #         # print info
# # # #         if (idx + 1) % opt.print_freq == 0:
# # # #             print('Train: [{0}][{1}/{2}]\t'
# # # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # # #                    data_time=data_time, loss=losses, top1=top1))
# # # #             sys.stdout.flush()
# # # #
# # # #     return losses.avg, top1.avg
# # # #
# # # #
# # # # def validate(val_loader, model, classifier, criterion, opt):
# # # #     """validation"""
# # # #     model.eval()
# # # #     classifier.eval()
# # # #
# # # #     batch_time = AverageMeter()
# # # #     losses = AverageMeter()
# # # #     top1 = AverageMeter()
# # # #
# # # #     with torch.no_grad():
# # # #         end = time.time()
# # # #         for idx, (images, labels) in enumerate(val_loader):
# # # #
# # # #
# # # #
# # # #             images = images.float().cuda()
# # # #             labels = labels.cuda()
# # # #             bsz = labels.shape[0]
# # # #
# # # #             # forward
# # # #             features=model(images)
# # # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # # #
# # # #             output = classifier(features,1)
# # # #
# # # #             # output1=classifier(features_64,64)
# # # #
# # # #             loss = criterion(output, labels)
# # # #
# # # #             # update metric
# # # #             losses.update(loss.item(), bsz)
# # # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # # #             top1.update(acc1[0], bsz)
# # # #
# # # #             # measure elapsed time
# # # #             batch_time.update(time.time() - end)
# # # #             end = time.time()
# # # #
# # # #             if idx % opt.print_freq == 0:
# # # #                 print('Test: [{0}/{1}]\t'
# # # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # # #                        idx, len(val_loader), batch_time=batch_time,
# # # #                        loss=losses, top1=top1))
# # # #
# # # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # # #     return losses.avg, top1.avg
# # # #
# # # #
# # # # def main():
# # # #     best_acc = 0
# # # #
# # # #
# # # #     for ep in range(1000,3250,250):
# # # #         opt = parse_option()
# # # #
# # # #         # build data loader
# # # #         train_loader, val_loader = set_loader(opt)
# # # #
# # # #         # build model and criterion
# # # #         model, classifier, criterion = set_model(opt)
# # # #
# # # #         # build optimizer
# # # #         optimizer = set_optimizer(opt, classifier)
# # # #
# # # #         # training routine
# # # #         for epoch in range(1, opt.epochs + 1):
# # # #             adjust_learning_rate(opt, optimizer, epoch)
# # # #
# # # #             # train for one epoch
# # # #             time1 = time.time()
# # # #             loss, acc = train(train_loader, model, classifier, criterion,
# # # #                               optimizer, epoch, opt)
# # # #             time2 = time.time()
# # # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # # #                 epoch, time2 - time1, acc))
# # # #
# # # #             # eval for one epoch
# # # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # # #             if val_acc > best_acc:
# # # #                 best_acc = val_acc
# # # #
# # # #         print('best accuracy: {:.2f}'.format(best_acc))
# # # #
# # # #
# # # # if __name__ == '__main__':
# # # #     main()
# # # #
# # # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar100',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_170.pth', #170
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
# # #     #                     help='path to pre-trained model')
# # #
# # #     opt = parser.parse_args()
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets1/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model(images)
# # #
# # #
# # #
# # #         ### 向量正交 2022.10.6
# # #         device = (torch.device('cuda')
# # #                   if features.is_cuda
# # #                   else torch.device('cpu'))
# # #
# # #         batch_size = features.shape[0]
# # #         labels = labels.contiguous().view(-1, 1)
# # #
# # #         mask = torch.eq(labels, labels.T).float().to(device)
# # #
# # #         contrast_count = 2
# # #         contrast_feature=features
# # #         # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         anchor_feature = contrast_feature
# # #         anchor_count = contrast_count
# # #         # tile mask
# # #         mask = mask.repeat(anchor_count, contrast_count)
# # #         # mask-out self-contrast cases
# # #         logits_mask = torch.scatter(
# # #             torch.ones_like(mask),
# # #             1,
# # #             torch.arange(batch_size).view(-1, 1).to(device),
# # #             0
# # #         )
# # #         # mask = mask * logits_mask
# # #
# # #         ### 计算同一类图片增强后差值的L2范数
# # #         # print(mask[0,:],mask.shape)
# # #         # print(mask[1,:],mask.shape)
# # #
# # #         num = 0
# # #         # print(mask[0:2,:5])
# # #         mask = mask * logits_mask
# # #         index_mask = mask[:, :] == 1
# # #         # print(index_mask[0:2,:5])
# # #
# # #         # sum_loss = torch.Tensor([0]).cuda()
# # #         # sum_loss_192 = torch.Tensor([0]).cuda()
# # #         sum_loss_92 = torch.Tensor([0]).cuda()
# # #         zj_zj = torch.Tensor([0]).cuda()
# # #         # div=torch.Tensor([0]).cuda()
# # #         # for i in range(len(mask)):
# # #         #     for index_feature in contrast_feature[index_mask[i,:]]:
# # #         #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
# # #         #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
# # #         #         sum_loss+=loss_c_192+loss_s_64
# # #         #         num+=1
# # #         import math
# # #
# # #         for i in range(len(mask)):
# # #
# # #             ### 引入 math
# # #             zhengjiao_fenzi=contrast_feature[i][:]*contrast_feature[index_mask[i, :]][:,:]
# # #             zhengjiao_fenmu=torch.norm(contrast_feature[i][:].unsqueeze(0),dim=1,keepdim=True)*torch.norm(contrast_feature[index_mask[i, :]][:, :],dim=1,keepdim=True)
# # #             zhengjiao_fenzi=torch.relu(zhengjiao_fenzi)
# # #             zhengjiao_fenmu=torch.relu(zhengjiao_fenmu)
# # #
# # #             zhengjiao=zhengjiao_fenzi/zhengjiao_fenmu
# # #             zj=torch.sum(zhengjiao).cuda()/len(contrast_feature[index_mask[i, :]])
# # #
# # #
# # #             e=torch.tensor(math.e,dtype=torch.float32).cuda()
# # #
# # #             # print(torch.pow(e,zj)-1)
# # #             zj_zj=zj_zj+torch.pow(e,zj)-1
# # #         # zj_zj/=mask.shape[0]
# # #         print(zj_zj)
# # #
# # #
# # #
# # #         # ###
# # #         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         features,features_s_64=torch.split(features,[192,64],dim=1)
# # #
# # #         labels = labels.contiguous().view(-1, 1)
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #         output = classifier(features.detach())
# # #
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)+zj_zj
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model(images)
# # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #
# # #
# # #     for ep in range(1000,3250,250):
# # #         opt = parse_option()
# # #
# # #         # build data loader
# # #         train_loader, val_loader = set_loader(opt)
# # #
# # #         # build model and criterion
# # #         model, classifier, criterion = set_model(opt)
# # #
# # #         # build optimizer
# # #         optimizer = set_optimizer(opt, classifier)
# # #
# # #         # training routine
# # #         for epoch in range(1, opt.epochs + 1):
# # #             adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #             # train for one epoch
# # #             time1 = time.time()
# # #             loss, acc = train(train_loader, model, classifier, criterion,
# # #                               optimizer, epoch, opt)
# # #             time2 = time.time()
# # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #                 epoch, time2 - time1, acc))
# # #
# # #             # eval for one epoch
# # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #             if val_acc > best_acc:
# # #                 best_acc = val_acc
# # #
# # #         print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# # #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_210.pth',
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # #     #                     help='path to pre-trained model')
# # #
# # #
# # #     opt = parser.parse_args()
# # #
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.encoder.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model.encoder(images)
# # #
# # #         # ###
# # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         #
# # #         #
# # #         #
# # #         #
# # #         #
# # #         #
# # #         #
# # #         # device = (torch.device('cuda')
# # #         #           if features.is_cuda
# # #         #           else torch.device('cpu'))
# # #         #
# # #         # batch_size = features.shape[0]/2
# # #         # labels = labels.contiguous().view(-1, 1)
# # #         # mask = torch.eq(labels, labels.T).float().to(device)
# # #         #
# # #         #
# # #         # # contrast_count = features.shape[1]
# # #         # # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         #
# # #         # # tile mask
# # #         # mask = mask.repeat(2,2)
# # #         # # mask-out self-contrast cases
# # #         # logits_mask = torch.scatter(
# # #         #     torch.ones_like(mask),
# # #         #     1,
# # #         #     torch.arange(batch_size*2).view(-1, 1).to(device).long(),
# # #         #     0
# # #         # )
# # #         # mask = mask * logits_mask
# # #         #
# # #         #
# # #         #
# # #         # ### 计算同一类图片增强后差值的L2范数
# # #         # # print(mask[0,:],mask.shape)
# # #         # # print(mask[1,:],mask.shape)
# # #         #
# # #         # num = 0
# # #         # # print(mask[0:2,:5])
# # #         # mask = mask * logits_mask
# # #         # index_mask = mask[:, :] == 1
# # #         # # print(index_mask[0:2,:5])
# # #         #
# # #         # # sum_loss = torch.Tensor([0]).cuda()
# # #         # # sum_loss_192 = torch.Tensor([0]).cuda()
# # #         # sum_loss_64 = torch.Tensor([0]).cuda()
# # #         # div=torch.Tensor([0]).cuda()
# # #         # # for i in range(len(mask)):
# # #         # #     for index_feature in contrast_feature[index_mask[i,:]]:
# # #         # #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
# # #         # #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
# # #         # #         sum_loss+=loss_c_192+loss_s_64
# # #         # #         num+=1
# # #         # for i in range(len(mask)):
# # #         #     # print("******")
# # #         #     # print((contrast_feature[i] - contrast_feature[index_mask[i, :]])[:].shape)
# # #         #     num += 1
# # #         #     # sum_loss_192 += torch.sum(
# # #         #     #     torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# # #         #     #                    len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         #     sum_loss_64 += torch.sum(
# # #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # #         #                        len(contrast_feature[index_mask[i, :]]))
# # #         #
# # #         #     c_192=F.normalize(torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]),dim=0),dim=1)
# # #         #     s_64=F.normalize(torch.cat((contrast_feature[i:i+1,192:],contrast_feature[index_mask[i,:]][:,192:]),dim=0).repeat(1,3),dim=1)
# # #         #
# # #         #     # print(torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1)))
# # #         #     div+=torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1))
# # #         #     # div+=torch.sum(torch.norm(,dim=0))-,dim=0).repeat(1,3)))/(len(contrast_feature[index_mask[i, :]])+1)
# # #         #     # print(div)
# # #         #
# # #         #     # print(torch.norm(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         #     # loss_c_192=torch.sum(torch.norm(contrast_feature[i]-contrast_feature[index_mask[i,:]]))
# # #         #     # print(loss_c_192)
# # #         # # sum_loss = sum_loss_192 / sum_loss_64
# # #         # # sum_loss_192/=num
# # #         # # sum_loss_64/=-num
# # #         # sum_loss_64=0.001*sum_loss_64
# # #         # div/=num
# # #         # div*=0.1
# # #         # # print(div)
# # #         # # print(sum_loss_64.data)
# # #         # # sum_loss/=num
# # #         # # print("sum_loss:", sum_loss)
# # #
# # #         # features_c_192, features_s_64= torch.split(contrast_feature, [192, 64], dim=1)
# # #
# # #
# # #
# # #         # ### 计算同一张图片增强后差值向量的l2范数
# # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         # loss_c_192 = torch.sum(torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1)) / len(
# # #         #     torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1))
# # #         # loss_s_64 = -torch.sum(torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1)) / len(
# # #         #     torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1))
# # #         # print(loss_c_192, loss_s_64)
# # #
# # #
# # #         # ### 计算同一类图片增强后差值向量的L2范数
# # #         #
# # #         # contrast_feature=features
# # #         # device = (torch.device('cuda')
# # #         #           if features.is_cuda
# # #         #           else torch.device('cpu'))
# # #
# # #         labels = labels.contiguous().view(-1, 1)
# # #         # mask = torch.eq(labels, labels.T).float().to(device)
# # #         # mask = mask.repeat(2, 2)
# # #         # num = 0
# # #         # index_mask = mask[:, :] == 1
# # #         # sum_loss_192 = torch.Tensor([0]).cuda()
# # #         # sum_loss_64 = torch.Tensor([0]).cuda()
# # #         # for i in range(len(mask)):
# # #         #     num += 1
# # #         #     sum_loss_192 += torch.sum(
# # #         #         torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# # #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         #     sum_loss_64 += torch.sum(
# # #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         # sum_loss_192/=num
# # #         # sum_loss_64/=-num
# # #         # print(sum_loss_192,sum_loss_64)
# # #         # print(contrast_feature.shape)
# # #         # features,features_64= torch.split(contrast_feature, [192, 64], dim=1)
# # #         # print(features.shape)
# # #
# # #         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #
# # #
# # #
# # #         output = classifier(features.detach())
# # #         # output1 = classifier(features_s_64.detach(),64)
# # #
# # #
# # #
# # #
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.encoder.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model.encoder(images)
# # #             # features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #     opt = parse_option()
# # #
# # #     # build data loader
# # #     train_loader, val_loader = set_loader(opt)
# # #
# # #     # build model and criterion
# # #     model, classifier, criterion = set_model(opt)
# # #
# # #     # build optimizer
# # #     optimizer = set_optimizer(opt, classifier)
# # #
# # #     # training routine
# # #     for epoch in range(1, opt.epochs + 1):
# # #         adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #         # train for one epoch
# # #         time1 = time.time()
# # #         loss, acc = train(train_loader, model, classifier, criterion,
# # #                           optimizer, epoch, opt)
# # #         time2 = time.time()
# # #         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #             epoch, time2 - time1, acc))
# # #
# # #         # eval for one epoch
# # #         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #         if val_acc > best_acc:
# # #             best_acc = val_acc
# # #
# # #     print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# # #
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # #     #                     help='path to pre-trained model')
# # #
# # #
# # #     opt = parser.parse_args()
# # #
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model(images)
# # #
# # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #
# # #
# # #
# # #         device = (torch.device('cuda')
# # #                   if features.is_cuda
# # #                   else torch.device('cpu'))
# # #
# # #
# # #
# # #         batch_size = features.shape[0]
# # #
# # #         labels = labels.contiguous().view(-1, 1)
# # #         if labels.shape[0] != batch_size:
# # #             raise ValueError('Num of labels does not match num of features')
# # #         mask = torch.eq(labels, labels.T).float().to(device)
# # #
# # #
# # #         contrast_count = features.shape[1]
# # #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #
# # #         # tile mask
# # #         mask = mask.repeat(2, contrast_count)
# # #         # mask-out self-contrast cases
# # #         logits_mask = torch.scatter(
# # #             torch.ones_like(mask),
# # #             1,
# # #             torch.arange(batch_size * 2).view(-1, 1).to(device),
# # #             0
# # #         )
# # #         mask = mask * logits_mask
# # #
# # #         ### 计算同一类图片增强后差值的L2范数
# # #         # print(mask[0,:],mask.shape)
# # #         # print(mask[1,:],mask.shape)
# # #
# # #         num = 0
# # #         mask = mask * logits_mask
# # #         index_mask = mask[:, :] == 1
# # #         sum_loss_64 = torch.Tensor([0]).cuda()
# # #         div = torch.Tensor([0]).cuda()
# # #         for i in range(len(mask)):
# # #             num += 1
# # #             sum_loss_64 += torch.sum(
# # #                 torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # #                                len(contrast_feature[index_mask[i, :]]))
# # #
# # #             c_192 = F.normalize(
# # #                 torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]), dim=0), dim=1)
# # #             s_64 = F.normalize(
# # #                 torch.cat((contrast_feature[i:i + 1, 192:], contrast_feature[index_mask[i, :]][:, 192:]), dim=0).repeat(
# # #                     1, 3), dim=1)
# # #
# # #             div += torch.sum(torch.norm(c_192 - s_64, dim=1)) / len(torch.norm(c_192 - s_64, dim=1))
# # #
# # #         sum_loss_64 = -0.001 * sum_loss_64
# # #         div /= -num
# # #         div *= 0.1
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #         # ###
# # #         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #
# # #         # labels = labels.contiguous().view(-1, 1)
# # #
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #         output = classifier(features.detach())
# # #
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)
# # #         loss=loss+div+sum_loss_64
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model(images)
# # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #
# # #
# # #     for ep in range(1000,3250,250):
# # #         opt = parse_option()
# # #
# # #         # build data loader
# # #         train_loader, val_loader = set_loader(opt)
# # #
# # #         # build model and criterion
# # #         model, classifier, criterion = set_model(opt)
# # #
# # #         # build optimizer
# # #         optimizer = set_optimizer(opt, classifier)
# # #
# # #         # training routine
# # #         for epoch in range(1, opt.epochs + 1):
# # #             adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #             # train for one epoch
# # #             time1 = time.time()
# # #             loss, acc = train(train_loader, model, classifier, criterion,
# # #                               optimizer, epoch, opt)
# # #             time2 = time.time()
# # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #                 epoch, time2 - time1, acc))
# # #
# # #             # eval for one epoch
# # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #             if val_acc > best_acc:
# # #                 best_acc = val_acc
# # #
# # #         print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# # #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# # #
# # #
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # #     #                     help='path to pre-trained model')
# # #
# # #
# # #     opt = parser.parse_args()
# # #
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model(images)
# # #
# # #
# # #         # ###
# # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# # #         labels = labels.contiguous().view(-1, 1)
# # #
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #         output = classifier(features.detach(),1)
# # #         output2=classifier(features_s_64.detach(),2)
# # #
# # #         output=output[:,9]+0.1*torch.sum(output2,dim=1).unsqueeze(1)
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)
# # #         # loss2=criterion(output2,labels)
# # #         # loss=loss-loss2
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model(images)
# # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features,1)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #
# # #
# # #     for ep in range(1000,3250,250):
# # #         opt = parse_option()
# # #
# # #         # build data loader
# # #         train_loader, val_loader = set_loader(opt)
# # #
# # #         # build model and criterion
# # #         model, classifier, criterion = set_model(opt)
# # #
# # #         # build optimizer
# # #         optimizer = set_optimizer(opt, classifier)
# # #
# # #         # training routine
# # #         for epoch in range(1, opt.epochs + 1):
# # #             adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #             # train for one epoch
# # #             time1 = time.time()
# # #             loss, acc = train(train_loader, model, classifier, criterion,
# # #                               optimizer, epoch, opt)
# # #             time2 = time.time()
# # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #                 epoch, time2 - time1, acc))
# # #
# # #             # eval for one epoch
# # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #             if val_acc > best_acc:
# # #                 best_acc = val_acc
# # #
# # #         print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# # #
# # #
# #
# #
# #
# #
# #
# #
# #
#
#
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar100',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_290.pth', #170
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
# #     #                     help='path to pre-trained model')
# #
# #     opt = parser.parse_args()
# #     # set the path according to the environment
# #     opt.data_folder = './datasets1/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #         # ###
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         features_a,features_b, features_s_64 = torch.split(features, [164,28, 64], dim=2)
# #         features_a = torch.cat(torch.unbind(features_a, dim=1), dim=0)
# #         features_b = torch.cat(torch.unbind(features_b,dim=1),dim=0)
# #         features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# #
# #         labels = labels.contiguous().view(-1, 1)
# #
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output_a = classifier(features_a.detach(),1)
# #         output_b=classifier(features_b.detach(),2)
# #         output_s_64=classifier(features_s_64.detach(),3)
# #
# #         # import math
# #         # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
# #         # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #         # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #         # b1=output_a*output_b/a1
# #         # b2=output_a*output_s_64/a2
# #         # b3=output_b*output_s_64/a3
# #         # e = torch.tensor(math.e, dtype=torch.float32).cuda()
# #         # c1=torch.pow(e,b1)
# #         # c2=torch.pow(e,b2)
# #         # c3=torch.pow(e,b3)
# #         # d = torch.sum(c1 + c2 + c3)/c1.shape[1]
# #
# #
# #
# #         output=output_a+output_b
# #
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model(images)
# #             features_a,features_b, features_s_64 = torch.split(features, [164,28, 64], dim=1)
# #
# #             output_a = classifier(features_a,1)
# #             output_b= classifier(features_b,2)
# #             output_s_64=classifier(features_s_64,3)
# #
# #             # import math
# #             # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
# #             # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #             # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #             # b1 = output_a * output_b / a1
# #             # b2 = output_a * output_s_64 / a2
# #             # b3 = output_b * output_s_64 / a3
# #             # e = torch.tensor(math.e, dtype=torch.float32).cuda()
# #             # c1 = torch.pow(e, b1)
# #             # c2 = torch.pow(e, b2)
# #             # c3 = torch.pow(e, b3)
# #             # d = torch.sum(c1 + c2 + c3)/c1.shape[1]
# #
# #
# #             output=output_a+output_b
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #
# #
# #     for ep in range(1000,3250,250):
# #         opt = parse_option()
# #
# #         # build data loader
# #         train_loader, val_loader = set_loader(opt)
# #
# #         # build model and criterion
# #         model, classifier, criterion = set_model(opt)
# #
# #         # build optimizer
# #         optimizer = set_optimizer(opt, classifier)
# #
# #         # training routine
# #         for epoch in range(1, opt.epochs + 1):
# #             adjust_learning_rate(opt, optimizer, epoch)
# #
# #             # train for one epoch
# #             time1 = time.time()
# #             loss, acc = train(train_loader, model, classifier, criterion,
# #                               optimizer, epoch, opt)
# #             time2 = time.time()
# #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #                 epoch, time2 - time1, acc))
# #
# #             # eval for one epoch
# #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #             if val_acc > best_acc:
# #                 best_acc = val_acc
# #
# #         print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
# #
#
#
#
#
#
#
#
#
# #
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar100',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_880.pth', #170
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
# #     #                     help='path to pre-trained model')
# #
# #     opt = parser.parse_args()
# #     # set the path according to the environment
# #     opt.data_folder = './datasets1/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #         # ###
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         features_a,_ = torch.split(features, [192,64], dim=2)
# #         _,features_b = torch.split(features, [164,92], dim=2)
# #         features_a = torch.cat(torch.unbind(features_a, dim=1), dim=0)
# #         features_b = torch.cat(torch.unbind(features_b,dim=1),dim=0)
# #         # features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# #
# #         labels = labels.contiguous().view(-1, 1)
# #
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output_a = classifier(features_a.detach(),1)
# #         output_b=classifier(features_b.detach(),2)
# #         # output_s_64=classifier(features_s_64.detach(),3)
# #
# #         import math
# #         a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
# #         # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #         # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #         b1=output_a*output_b/a1
# #         # b2=output_a*output_s_64/a2
# #         # b3=output_b*output_s_64/a3
# #         e = torch.tensor(math.e, dtype=torch.float32).cuda()
# #         c1=torch.pow(e,b1)
# #         # c2=torch.pow(e,b2)
# #         # c3=torch.pow(e,b3)
# #         d=torch.sum(c1)
# #         # d = torch.sum(c1 + c2 + c3)/c1.shape[1]
# #
# #
# #
# #         output=output_a+output_b
# #
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)*d
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model(images)
# #             features_a,_ = torch.split(features, [192,64], dim=1)
# #             _,features_b = torch.split(features, [164,92], dim=1)
# #
# #             output_a = classifier(features_a,1)
# #             output_b= classifier(features_b,2)
# #             # output_s_64=classifier(features_s_64,3)
# #
# #             import math
# #             a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
# #             # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #             # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_s_64, dim=1, keepdim=True)
# #             b1 = output_a * output_b / a1
# #             # b2 = output_a * output_s_64 / a2
# #             # b3 = output_b * output_s_64 / a3
# #             e = torch.tensor(math.e, dtype=torch.float32).cuda()
# #             c1 = torch.pow(e, b1)
# #             # c2 = torch.pow(e, b2)
# #             # c3 = torch.pow(e, b3)
# #             d=torch.sum(c1)
# #             # d = torch.sum(c1 + c2 + c3)/c1.shape[1]
# #
# #
# #             output=output_a+output_b
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)*d
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #
# #
# #     for ep in range(1000,3250,250):
# #         opt = parse_option()
# #
# #         # build data loader
# #         train_loader, val_loader = set_loader(opt)
# #
# #         # build model and criterion
# #         model, classifier, criterion = set_model(opt)
# #
# #         # build optimizer
# #         optimizer = set_optimizer(opt, classifier)
# #
# #         # training routine
# #         for epoch in range(1, opt.epochs + 1):
# #             adjust_learning_rate(opt, optimizer, epoch)
# #
# #             # train for one epoch
# #             time1 = time.time()
# #             loss, acc = train(train_loader, model, classifier, criterion,
# #                               optimizer, epoch, opt)
# #             time2 = time.time()
# #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #                 epoch, time2 - time1, acc))
# #
# #             # eval for one epoch
# #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #             if val_acc > best_acc:
# #                 best_acc = val_acc
# #
# #         print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#

#
# from __future__ import print_function
#
# import sys
# import argparse
# import time
# import math
#
# import torch
# import torch.backends.cudnn as cudnn
#
# from main_ce import set_loader
# from util import AverageMeter
# from util import adjust_learning_rate, warmup_learning_rate, accuracy
# from util import set_optimizer
# from networks.resnet_big import SupConResNet, LinearClassifier
#
#
#
# import torch.nn.functional as F
#
# try:
#     import apex
#     from apex import amp, optimizers
# except ImportError:
#     pass
#
#
# def parse_option():
#     parser = argparse.ArgumentParser('argument for training')
#
#     parser.add_argument('--print_freq', type=int, default=10,
#                         help='print frequency')
#     parser.add_argument('--save_freq', type=int, default=50,
#                         help='save frequency')
#     parser.add_argument('--batch_size', type=int, default=256,# 256
#                         help='batch_size')
#     parser.add_argument('--num_workers', type=int, default=4,
#                         help='num of workers to use')
#     parser.add_argument('--epochs', type=int, default=100,
#                         help='number of training epochs')
#
#     # optimization
#     parser.add_argument('--learning_rate', type=float, default=0.1,
#                         help='learning rate')
#     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90', # 60,75,90
#                         help='where to decay lr, can be a list')
#     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
#                         help='decay rate for learning rate')
#     parser.add_argument('--weight_decay', type=float, default=0,
#                         help='weight decay')
#     parser.add_argument('--momentum', type=float, default=0.9,
#                         help='momentum')
#
#     # model dataset
#     parser.add_argument('--model', type=str, default='resnet50')
#     parser.add_argument('--dataset', type=str, default='cifar100',
#                         choices=['cifar10', 'cifar100'], help='dataset')
#
#     # other setting
#     # parser.add_argument('--cosine', action='store_true',
#     #                     help='using cosine annealing')
#     parser.add_argument('--cosine', default='true',
#                         help='using cosine annealing')
#     parser.add_argument('--warm', action='store_true',
#                         help='warm-up for large batch training')
#
#
#     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_200.pth', #170
#                         help='path to pre-trained model')
#
#     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
#     #                     help='path to pre-trained model')
#
#     opt = parser.parse_args()
#     # set the path according to the environment
#     opt.data_folder = './datasets1/'
#
#     iterations = opt.lr_decay_epochs.split(',')
#     opt.lr_decay_epochs = list([])
#     for it in iterations:
#         opt.lr_decay_epochs.append(int(it))
#
#     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
#         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
#                opt.batch_size)
#
#     if opt.cosine:
#         opt.model_name = '{}_cosine'.format(opt.model_name)
#
#     # warm-up for large-batch training,
#     if opt.warm:
#         opt.model_name = '{}_warm'.format(opt.model_name)
#         opt.warmup_from = 0.01
#         opt.warm_epochs = 10
#         if opt.cosine:
#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
#         else:
#             opt.warmup_to = opt.learning_rate
#
#     if opt.dataset == 'cifar10':
#         opt.n_cls = 10
#     elif opt.dataset == 'cifar100':
#         opt.n_cls = 100
#     else:
#         raise ValueError('dataset not supported: {}'.format(opt.dataset))
#
#     return opt
#
#
# def set_model(opt):
#     model = SupConResNet(name=opt.model)
#     criterion = torch.nn.CrossEntropyLoss()
#
#     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
#
#     ckpt = torch.load(opt.ckpt, map_location='cpu')
#     state_dict = ckpt['model']
#
#     if torch.cuda.is_available():
#         if torch.cuda.device_count() > 1:
#             model = torch.nn.DataParallel(model)
#         else:
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k = k.replace("module.", "")
#                 new_state_dict[k] = v
#             state_dict = new_state_dict
#         model = model.cuda()
#         classifier = classifier.cuda()
#         criterion = criterion.cuda()
#         cudnn.benchmark = True
#
#         model.load_state_dict(state_dict)
#
#     return model, classifier, criterion
#
#
# def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
#     """one epoch training"""
#     model.eval()
#     classifier.train()
#
#     batch_time = AverageMeter()
#     data_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     end = time.time()
#     for idx, (images, labels) in enumerate(train_loader):
#         images = torch.cat([images[0], images[1]], dim=0)
#
#         data_time.update(time.time() - end)
#
#         images = images.cuda(non_blocking=True)
#         labels = labels.cuda(non_blocking=True)
#         bsz = labels.shape[0]
#
#         # warm-up learning rate
#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
#
#         # compute loss
#         with torch.no_grad():
#             features = model(images)
#
#         # # ###
#         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         # # features_a,_ = torch.split(features, [192,64], dim=2)
#         # # _,features_b = torch.split(features, [164,92], dim=2)
#         # features_a,features_b,features_c=torch.split(features,[164,28,64],dim=2)
#         # features_a = torch.cat(torch.unbind(features_a, dim=1), dim=0)
#         # features_b = torch.cat(torch.unbind(features_b, dim=1), dim=0)
#         # features_c = torch.cat(torch.unbind(features_c, dim=1), dim=0)
#         #
#         # features_n,_=torch.split(features, [192, 64], dim=2)
#         # features_new=torch.cat(torch.unbind(features_n,dim=1),dim=0)
#         #
#         # output_a = classifier(features_a.detach(),1)
#         # output_b = classifier(features_b.detach(),2)
#         # output_c = classifier(features_c.detach(),3)
#         #
#         # import math
#         # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#         # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         # b1 = output_a * output_b / a1
#         # b2 = output_a * output_c / a2
#         # b3 = output_b * output_c / a3
#         # e = torch.tensor(math.e, dtype=torch.float32).cuda()
#         # c1 = torch.pow(e, b1)
#         # c2 = torch.pow(e, b2)
#         # c3 = torch.pow(e, b3)
#         # d = torch.sum(c1+c2+c3)
#         # ###
#         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         # features_a,_ = torch.split(features, [192,64], dim=2)
#         # _,features_b = torch.split(features, [164,92], dim=2)
#         # features_a, _ = torch.split(features, [192, 64], dim=2)
#         # _, features_c = torch.split(features, [192, 64], dim=2)
#         features_a,features_b,features_c=torch.split(features,[164,28,64],dim=2)
#
#
#         features_a = torch.cat(torch.unbind(features_a, dim=1), dim=0)
#         features_b = torch.cat(torch.unbind(features_b, dim=1), dim=0)
#         features_c = torch.cat(torch.unbind(features_c, dim=1), dim=0)
#
#
#
#         output_a = classifier(features_a.detach(), 1)
#         output_b = classifier(features_b.detach(), 2)
#         output_c = classifier(features_c.detach(), 3)
#
#
#         features_n, _ = torch.split(features, [256,0], dim=2)
#         # features_n=torch.cat([output_a,output_b,output_c],dim=1)
#         features_new = torch.cat(torch.unbind(features_n, dim=1), dim=0)
#
#
#         ### 特征向量非负
#         output_a = torch.relu(output_a)
#         output_b=torch.relu(output_b)
#         output_c = torch.relu(output_c)
#
#         import math
#         e = torch.tensor(math.e, dtype=torch.float32).cuda()
#
#         a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#         b1 = torch.sum(output_a * output_b, dim=1 , keepdim=True) / a1
#         c1=torch.pow(e,b1)-1
#
#         a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         b2 = torch.sum(output_a * output_c, dim=1,keepdim=True) / a2
#         c2 = torch.pow(e, b2) - 1
#
#         a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         b3 = torch.sum(output_b * output_c, dim=1, keepdim=True) / a3
#         c3 = torch.pow(e, b3) - 1
#
#         d = torch.sum(c1)+torch.sum(c2)+torch.sum(c3)
#
#
#         labels = labels.contiguous().view(-1, 1)
#
#
#         labels = torch.cat((labels, labels), dim=0)
#
#         output = classifier(features_new.detach())
#
#
#         labels=labels.squeeze(1)
#         loss = criterion(output, labels)+d
#
#         # update metric
#         losses.update(loss.item(), bsz)
#         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#         top1.update(acc1[0], bsz)
#
#         # SGD
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # measure elapsed time
#         batch_time.update(time.time() - end)
#         end = time.time()
#
#         # print info
#         if (idx + 1) % opt.print_freq == 0:
#             print('Train: [{0}][{1}/{2}]\t'
#                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
#                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
#                    data_time=data_time, loss=losses, top1=top1))
#             sys.stdout.flush()
#
#     return losses.avg, top1.avg
#
#
# def validate(val_loader, model, classifier, criterion, opt):
#     """validation"""
#     model.eval()
#     classifier.eval()
#
#     batch_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     with torch.no_grad():
#         end = time.time()
#         for idx, (images, labels) in enumerate(val_loader):
#
#
#
#             images = images.float().cuda()
#             labels = labels.cuda()
#             bsz = labels.shape[0]
#
#             # # forward
#             # features=model(images)
#             # features_a, features_b, features_c = torch.split(features, [164, 28, 64], dim=1)
#             #
#             # output_a = classifier(features_a.detach(), 1)
#             # output_b = classifier(features_b.detach(), 2)
#             # output_c = classifier(features_c.detach(), 3)
#             #
#             # import math
#             # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#             # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             # b1 = output_a * output_b / a1
#             # b2 = output_a * output_c / a2
#             # b3 = output_b * output_c / a3
#             # e = torch.tensor(math.e, dtype=torch.float32).cuda()
#             # c1 = torch.pow(e, b1)
#             # c2 = torch.pow(e, b2)
#             # c3 = torch.pow(e, b3)
#             # d = torch.sum(c1 + c2 + c3)
#             #
#             # features_n, _ = torch.split(features, [192, 64], dim=1)
#             # output = classifier(features_n.detach())
#
#             # forward
#             features = model(images)
#             features_a, features_b, features_c = torch.split(features, [164, 28, 64], dim=1)
#
#
#             output_a = classifier(features_a.detach(), 1)
#             output_b = classifier(features_b.detach(), 2)
#             output_c = classifier(features_c.detach(), 3)
#
#             features_new, _ = torch.split(features, [256,0], dim=1)
#             # features_n = torch.cat([output_a, output_b, output_c], dim=1)
#             # features_new = torch.cat(torch.unbind(features_n, dim=1), dim=0)
#
#             ### 特征向量非负
#             output_a = torch.relu(output_a)
#             output_b=torch.relu(output_b)
#             output_c = torch.relu(output_c)
#
#             import math
#             e = torch.tensor(math.e, dtype=torch.float32).cuda()
#
#             a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#             b1 = torch.sum(output_a * output_b, dim=1, keepdim=True) / a1
#             c1 = torch.pow(e, b1) - 1
#
#             a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             b2 = torch.sum(output_a * output_c, dim=1, keepdim=True) / a2
#             c2 = torch.pow(e, b2) - 1
#
#             a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             b3 = torch.sum(output_b * output_c, dim=1, keepdim=True) / a3
#             c3 = torch.pow(e, b3) - 1
#             # d = torch.sum(c1 + c2 + c3)
#             d=torch.sum(c1)+torch.sum(c2)+torch.sum(c3)
#             output = classifier(features_new.detach())
#
#
#             # output1=classifier(features_64,64)
#
#             loss = criterion(output, labels)+d
#
#             # update metric
#             losses.update(loss.item(), bsz)
#             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#             top1.update(acc1[0], bsz)
#
#             # measure elapsed time
#             batch_time.update(time.time() - end)
#             end = time.time()
#
#             if idx % opt.print_freq == 0:
#                 print('Test: [{0}/{1}]\t'
#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                        idx, len(val_loader), batch_time=batch_time,
#                        loss=losses, top1=top1))
#
#     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
#     return losses.avg, top1.avg
#
#
# def main():
#     best_acc = 0
#
#
#     for ep in range(1000,3250,250):
#         opt = parse_option()
#
#         # build data loader
#         train_loader, val_loader = set_loader(opt)
#
#         # build model and criterion
#         model, classifier, criterion = set_model(opt)
#
#         # build optimizer
#         optimizer = set_optimizer(opt, classifier)
#
#         # training routine
#         for epoch in range(1, opt.epochs + 1):
#             adjust_learning_rate(opt, optimizer, epoch)
#
#             # train for one epoch
#             time1 = time.time()
#             loss, acc = train(train_loader, model, classifier, criterion,
#                               optimizer, epoch, opt)
#             time2 = time.time()
#             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
#                 epoch, time2 - time1, acc))
#
#             # eval for one epoch
#             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
#             if val_acc > best_acc:
#                 best_acc = val_acc
#
#         print('best accuracy: {:.2f}'.format(best_acc))
#
#
# if __name__ == '__main__':
#     main()


#
#
#
#
#
#
#
#
#
#
#
#
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_210.pth',
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # #     #                     help='path to pre-trained model')
# # #
# # #
# # #     opt = parser.parse_args()
# # #
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.encoder.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model.encoder(images)
# # #
# # #         # ###
# # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         #
# # #         #
# # #         #
# # #         #
# # #         #
# # #         #
# # #         #
# # #         # device = (torch.device('cuda')
# # #         #           if features.is_cuda
# # #         #           else torch.device('cpu'))
# # #         #
# # #         # batch_size = features.shape[0]/2
# # #         # labels = labels.contiguous().view(-1, 1)
# # #         # mask = torch.eq(labels, labels.T).float().to(device)
# # #         #
# # #         #
# # #         # # contrast_count = features.shape[1]
# # #         # # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         #
# # #         # # tile mask
# # #         # mask = mask.repeat(2,2)
# # #         # # mask-out self-contrast cases
# # #         # logits_mask = torch.scatter(
# # #         #     torch.ones_like(mask),
# # #         #     1,
# # #         #     torch.arange(batch_size*2).view(-1, 1).to(device).long(),
# # #         #     0
# # #         # )
# # #         # mask = mask * logits_mask
# # #         #
# # #         #
# # #         #
# # #         # ### 计算同一类图片增强后差值的L2范数
# # #         # # print(mask[0,:],mask.shape)
# # #         # # print(mask[1,:],mask.shape)
# # #         #
# # #         # num = 0
# # #         # # print(mask[0:2,:5])
# # #         # mask = mask * logits_mask
# # #         # index_mask = mask[:, :] == 1
# # #         # # print(index_mask[0:2,:5])
# # #         #
# # #         # # sum_loss = torch.Tensor([0]).cuda()
# # #         # # sum_loss_192 = torch.Tensor([0]).cuda()
# # #         # sum_loss_64 = torch.Tensor([0]).cuda()
# # #         # div=torch.Tensor([0]).cuda()
# # #         # # for i in range(len(mask)):
# # #         # #     for index_feature in contrast_feature[index_mask[i,:]]:
# # #         # #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
# # #         # #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
# # #         # #         sum_loss+=loss_c_192+loss_s_64
# # #         # #         num+=1
# # #         # for i in range(len(mask)):
# # #         #     # print("******")
# # #         #     # print((contrast_feature[i] - contrast_feature[index_mask[i, :]])[:].shape)
# # #         #     num += 1
# # #         #     # sum_loss_192 += torch.sum(
# # #         #     #     torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# # #         #     #                    len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         #     sum_loss_64 += torch.sum(
# # #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # #         #                        len(contrast_feature[index_mask[i, :]]))
# # #         #
# # #         #     c_192=F.normalize(torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]),dim=0),dim=1)
# # #         #     s_64=F.normalize(torch.cat((contrast_feature[i:i+1,192:],contrast_feature[index_mask[i,:]][:,192:]),dim=0).repeat(1,3),dim=1)
# # #         #
# # #         #     # print(torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1)))
# # #         #     div+=torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1))
# # #         #     # div+=torch.sum(torch.norm(,dim=0))-,dim=0).repeat(1,3)))/(len(contrast_feature[index_mask[i, :]])+1)
# # #         #     # print(div)
# # #         #
# # #         #     # print(torch.norm(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         #     # loss_c_192=torch.sum(torch.norm(contrast_feature[i]-contrast_feature[index_mask[i,:]]))
# # #         #     # print(loss_c_192)
# # #         # # sum_loss = sum_loss_192 / sum_loss_64
# # #         # # sum_loss_192/=num
# # #         # # sum_loss_64/=-num
# # #         # sum_loss_64=0.001*sum_loss_64
# # #         # div/=num
# # #         # div*=0.1
# # #         # # print(div)
# # #         # # print(sum_loss_64.data)
# # #         # # sum_loss/=num
# # #         # # print("sum_loss:", sum_loss)
# # #
# # #         # features_c_192, features_s_64= torch.split(contrast_feature, [192, 64], dim=1)
# # #
# # #
# # #
# # #         # ### 计算同一张图片增强后差值向量的l2范数
# # #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         # loss_c_192 = torch.sum(torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1)) / len(
# # #         #     torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1))
# # #         # loss_s_64 = -torch.sum(torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1)) / len(
# # #         #     torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1))
# # #         # print(loss_c_192, loss_s_64)
# # #
# # #
# # #         # ### 计算同一类图片增强后差值向量的L2范数
# # #         #
# # #         # contrast_feature=features
# # #         # device = (torch.device('cuda')
# # #         #           if features.is_cuda
# # #         #           else torch.device('cpu'))
# # #
# # #         labels = labels.contiguous().view(-1, 1)
# # #         # mask = torch.eq(labels, labels.T).float().to(device)
# # #         # mask = mask.repeat(2, 2)
# # #         # num = 0
# # #         # index_mask = mask[:, :] == 1
# # #         # sum_loss_192 = torch.Tensor([0]).cuda()
# # #         # sum_loss_64 = torch.Tensor([0]).cuda()
# # #         # for i in range(len(mask)):
# # #         #     num += 1
# # #         #     sum_loss_192 += torch.sum(
# # #         #         torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# # #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         #     sum_loss_64 += torch.sum(
# # #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# # #         # sum_loss_192/=num
# # #         # sum_loss_64/=-num
# # #         # print(sum_loss_192,sum_loss_64)
# # #         # print(contrast_feature.shape)
# # #         # features,features_64= torch.split(contrast_feature, [192, 64], dim=1)
# # #         # print(features.shape)
# # #
# # #         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #
# # #
# # #
# # #         output = classifier(features.detach())
# # #         # output1 = classifier(features_s_64.detach(),64)
# # #
# # #
# # #
# # #
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.encoder.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model.encoder(images)
# # #             # features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #     opt = parse_option()
# # #
# # #     # build data loader
# # #     train_loader, val_loader = set_loader(opt)
# # #
# # #     # build model and criterion
# # #     model, classifier, criterion = set_model(opt)
# # #
# # #     # build optimizer
# # #     optimizer = set_optimizer(opt, classifier)
# # #
# # #     # training routine
# # #     for epoch in range(1, opt.epochs + 1):
# # #         adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #         # train for one epoch
# # #         time1 = time.time()
# # #         loss, acc = train(train_loader, model, classifier, criterion,
# # #                           optimizer, epoch, opt)
# # #         time2 = time.time()
# # #         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #             epoch, time2 - time1, acc))
# # #
# # #         # eval for one epoch
# # #         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #         if val_acc > best_acc:
# # #             best_acc = val_acc
# # #
# # #     print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# # #
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # #     #                     help='path to pre-trained model')
# # #
# # #
# # #     opt = parser.parse_args()
# # #
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model(images)
# # #
# # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #
# # #
# # #
# # #         device = (torch.device('cuda')
# # #                   if features.is_cuda
# # #                   else torch.device('cpu'))
# # #
# # #
# # #
# # #         batch_size = features.shape[0]
# # #
# # #         labels = labels.contiguous().view(-1, 1)
# # #         if labels.shape[0] != batch_size:
# # #             raise ValueError('Num of labels does not match num of features')
# # #         mask = torch.eq(labels, labels.T).float().to(device)
# # #
# # #
# # #         contrast_count = features.shape[1]
# # #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #
# # #         # tile mask
# # #         mask = mask.repeat(2, contrast_count)
# # #         # mask-out self-contrast cases
# # #         logits_mask = torch.scatter(
# # #             torch.ones_like(mask),
# # #             1,
# # #             torch.arange(batch_size * 2).view(-1, 1).to(device),
# # #             0
# # #         )
# # #         mask = mask * logits_mask
# # #
# # #         ### 计算同一类图片增强后差值的L2范数
# # #         # print(mask[0,:],mask.shape)
# # #         # print(mask[1,:],mask.shape)
# # #
# # #         num = 0
# # #         mask = mask * logits_mask
# # #         index_mask = mask[:, :] == 1
# # #         sum_loss_64 = torch.Tensor([0]).cuda()
# # #         div = torch.Tensor([0]).cuda()
# # #         for i in range(len(mask)):
# # #             num += 1
# # #             sum_loss_64 += torch.sum(
# # #                 torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# # #                                len(contrast_feature[index_mask[i, :]]))
# # #
# # #             c_192 = F.normalize(
# # #                 torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]), dim=0), dim=1)
# # #             s_64 = F.normalize(
# # #                 torch.cat((contrast_feature[i:i + 1, 192:], contrast_feature[index_mask[i, :]][:, 192:]), dim=0).repeat(
# # #                     1, 3), dim=1)
# # #
# # #             div += torch.sum(torch.norm(c_192 - s_64, dim=1)) / len(torch.norm(c_192 - s_64, dim=1))
# # #
# # #         sum_loss_64 = -0.001 * sum_loss_64
# # #         div /= -num
# # #         div *= 0.1
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #
# # #         # ###
# # #         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #
# # #         # labels = labels.contiguous().view(-1, 1)
# # #
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #         output = classifier(features.detach())
# # #
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)
# # #         loss=loss+div+sum_loss_64
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model(images)
# # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #
# # #
# # #     for ep in range(1000,3250,250):
# # #         opt = parse_option()
# # #
# # #         # build data loader
# # #         train_loader, val_loader = set_loader(opt)
# # #
# # #         # build model and criterion
# # #         model, classifier, criterion = set_model(opt)
# # #
# # #         # build optimizer
# # #         optimizer = set_optimizer(opt, classifier)
# # #
# # #         # training routine
# # #         for epoch in range(1, opt.epochs + 1):
# # #             adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #             # train for one epoch
# # #             time1 = time.time()
# # #             loss, acc = train(train_loader, model, classifier, criterion,
# # #                               optimizer, epoch, opt)
# # #             time2 = time.time()
# # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #                 epoch, time2 - time1, acc))
# # #
# # #             # eval for one epoch
# # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #             if val_acc > best_acc:
# # #                 best_acc = val_acc
# # #
# # #         print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# # #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# # #
# # #
# # # from __future__ import print_function
# # #
# # # import sys
# # # import argparse
# # # import time
# # # import math
# # #
# # # import torch
# # # import torch.backends.cudnn as cudnn
# # #
# # # from main_ce import set_loader
# # # from util import AverageMeter
# # # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # # from util import set_optimizer
# # # from networks.resnet_big import SupConResNet, LinearClassifier
# # #
# # #
# # #
# # # import torch.nn.functional as F
# # #
# # # try:
# # #     import apex
# # #     from apex import amp, optimizers
# # # except ImportError:
# # #     pass
# # #
# # #
# # # def parse_option():
# # #     parser = argparse.ArgumentParser('argument for training')
# # #
# # #     parser.add_argument('--print_freq', type=int, default=10,
# # #                         help='print frequency')
# # #     parser.add_argument('--save_freq', type=int, default=50,
# # #                         help='save frequency')
# # #     parser.add_argument('--batch_size', type=int, default=256,# 256
# # #                         help='batch_size')
# # #     parser.add_argument('--num_workers', type=int, default=4,
# # #                         help='num of workers to use')
# # #     parser.add_argument('--epochs', type=int, default=100,
# # #                         help='number of training epochs')
# # #
# # #     # optimization
# # #     parser.add_argument('--learning_rate', type=float, default=0.1,
# # #                         help='learning rate')
# # #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# # #                         help='where to decay lr, can be a list')
# # #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# # #                         help='decay rate for learning rate')
# # #     parser.add_argument('--weight_decay', type=float, default=0,
# # #                         help='weight decay')
# # #     parser.add_argument('--momentum', type=float, default=0.9,
# # #                         help='momentum')
# # #
# # #     # model dataset
# # #     parser.add_argument('--model', type=str, default='resnet50')
# # #     parser.add_argument('--dataset', type=str, default='cifar10',
# # #                         choices=['cifar10', 'cifar100'], help='dataset')
# # #
# # #     # other setting
# # #     # parser.add_argument('--cosine', action='store_true',
# # #     #                     help='using cosine annealing')
# # #     parser.add_argument('--cosine', default='true',
# # #                         help='using cosine annealing')
# # #     parser.add_argument('--warm', action='store_true',
# # #                         help='warm-up for large batch training')
# # #
# # #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# # #                         help='path to pre-trained model')
# # #
# # #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# # #     #                     help='path to pre-trained model')
# # #
# # #
# # #     opt = parser.parse_args()
# # #
# # #     # set the path according to the environment
# # #     opt.data_folder = './datasets/'
# # #
# # #     iterations = opt.lr_decay_epochs.split(',')
# # #     opt.lr_decay_epochs = list([])
# # #     for it in iterations:
# # #         opt.lr_decay_epochs.append(int(it))
# # #
# # #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# # #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# # #                opt.batch_size)
# # #
# # #     if opt.cosine:
# # #         opt.model_name = '{}_cosine'.format(opt.model_name)
# # #
# # #     # warm-up for large-batch training,
# # #     if opt.warm:
# # #         opt.model_name = '{}_warm'.format(opt.model_name)
# # #         opt.warmup_from = 0.01
# # #         opt.warm_epochs = 10
# # #         if opt.cosine:
# # #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# # #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# # #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# # #         else:
# # #             opt.warmup_to = opt.learning_rate
# # #
# # #     if opt.dataset == 'cifar10':
# # #         opt.n_cls = 10
# # #     elif opt.dataset == 'cifar100':
# # #         opt.n_cls = 100
# # #     else:
# # #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# # #
# # #     return opt
# # #
# # #
# # # def set_model(opt):
# # #     model = SupConResNet(name=opt.model)
# # #     criterion = torch.nn.CrossEntropyLoss()
# # #
# # #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# # #
# # #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# # #     state_dict = ckpt['model']
# # #
# # #     if torch.cuda.is_available():
# # #         if torch.cuda.device_count() > 1:
# # #             model = torch.nn.DataParallel(model)
# # #         else:
# # #             new_state_dict = {}
# # #             for k, v in state_dict.items():
# # #                 k = k.replace("module.", "")
# # #                 new_state_dict[k] = v
# # #             state_dict = new_state_dict
# # #         model = model.cuda()
# # #         classifier = classifier.cuda()
# # #         criterion = criterion.cuda()
# # #         cudnn.benchmark = True
# # #
# # #         model.load_state_dict(state_dict)
# # #
# # #     return model, classifier, criterion
# # #
# # #
# # # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# # #     """one epoch training"""
# # #     model.eval()
# # #     classifier.train()
# # #
# # #     batch_time = AverageMeter()
# # #     data_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     end = time.time()
# # #     for idx, (images, labels) in enumerate(train_loader):
# # #         images = torch.cat([images[0], images[1]], dim=0)
# # #
# # #         data_time.update(time.time() - end)
# # #
# # #         images = images.cuda(non_blocking=True)
# # #         labels = labels.cuda(non_blocking=True)
# # #         bsz = labels.shape[0]
# # #
# # #         # warm-up learning rate
# # #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# # #
# # #         # compute loss
# # #         with torch.no_grad():
# # #             features = model(images)
# # #
# # #
# # #         # ###
# # #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# # #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# # #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# # #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# # #         features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# # #         labels = labels.contiguous().view(-1, 1)
# # #
# # #
# # #         labels = torch.cat((labels, labels), dim=0)
# # #
# # #         output = classifier(features.detach(),1)
# # #         output2=classifier(features_s_64.detach(),2)
# # #
# # #         output=output[:,9]+0.1*torch.sum(output2,dim=1).unsqueeze(1)
# # #
# # #         labels=labels.squeeze(1)
# # #         loss = criterion(output, labels)
# # #         # loss2=criterion(output2,labels)
# # #         # loss=loss-loss2
# # #
# # #         # update metric
# # #         losses.update(loss.item(), bsz)
# # #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #         top1.update(acc1[0], bsz)
# # #
# # #         # SGD
# # #         optimizer.zero_grad()
# # #         loss.backward()
# # #         optimizer.step()
# # #
# # #         # measure elapsed time
# # #         batch_time.update(time.time() - end)
# # #         end = time.time()
# # #
# # #         # print info
# # #         if (idx + 1) % opt.print_freq == 0:
# # #             print('Train: [{0}][{1}/{2}]\t'
# # #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# # #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# # #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# # #                    data_time=data_time, loss=losses, top1=top1))
# # #             sys.stdout.flush()
# # #
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def validate(val_loader, model, classifier, criterion, opt):
# # #     """validation"""
# # #     model.eval()
# # #     classifier.eval()
# # #
# # #     batch_time = AverageMeter()
# # #     losses = AverageMeter()
# # #     top1 = AverageMeter()
# # #
# # #     with torch.no_grad():
# # #         end = time.time()
# # #         for idx, (images, labels) in enumerate(val_loader):
# # #
# # #
# # #
# # #             images = images.float().cuda()
# # #             labels = labels.cuda()
# # #             bsz = labels.shape[0]
# # #
# # #             # forward
# # #             features=model(images)
# # #             features, features_64 = torch.split(features, [192, 64], dim=1)
# # #
# # #             output = classifier(features,1)
# # #
# # #             # output1=classifier(features_64,64)
# # #
# # #             loss = criterion(output, labels)
# # #
# # #             # update metric
# # #             losses.update(loss.item(), bsz)
# # #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# # #             top1.update(acc1[0], bsz)
# # #
# # #             # measure elapsed time
# # #             batch_time.update(time.time() - end)
# # #             end = time.time()
# # #
# # #             if idx % opt.print_freq == 0:
# # #                 print('Test: [{0}/{1}]\t'
# # #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# # #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# # #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# # #                        idx, len(val_loader), batch_time=batch_time,
# # #                        loss=losses, top1=top1))
# # #
# # #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# # #     return losses.avg, top1.avg
# # #
# # #
# # # def main():
# # #     best_acc = 0
# # #
# # #
# # #     for ep in range(1000,3250,250):
# # #         opt = parse_option()
# # #
# # #         # build data loader
# # #         train_loader, val_loader = set_loader(opt)
# # #
# # #         # build model and criterion
# # #         model, classifier, criterion = set_model(opt)
# # #
# # #         # build optimizer
# # #         optimizer = set_optimizer(opt, classifier)
# # #
# # #         # training routine
# # #         for epoch in range(1, opt.epochs + 1):
# # #             adjust_learning_rate(opt, optimizer, epoch)
# # #
# # #             # train for one epoch
# # #             time1 = time.time()
# # #             loss, acc = train(train_loader, model, classifier, criterion,
# # #                               optimizer, epoch, opt)
# # #             time2 = time.time()
# # #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# # #                 epoch, time2 - time1, acc))
# # #
# # #             # eval for one epoch
# # #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# # #             if val_acc > best_acc:
# # #                 best_acc = val_acc
# # #
# # #         print('best accuracy: {:.2f}'.format(best_acc))
# # #
# # #
# # # if __name__ == '__main__':
# # #     main()
# # #
# # #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar100',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_170.pth', #170
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
# #     #                     help='path to pre-trained model')
# #
# #     opt = parser.parse_args()
# #     # set the path according to the environment
# #     opt.data_folder = './datasets1/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #
# #
# #         ### 向量正交 2022.10.6
# #         device = (torch.device('cuda')
# #                   if features.is_cuda
# #                   else torch.device('cpu'))
# #
# #         batch_size = features.shape[0]
# #         labels = labels.contiguous().view(-1, 1)
# #
# #         mask = torch.eq(labels, labels.T).float().to(device)
# #
# #         contrast_count = 2
# #         contrast_feature=features
# #         # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# #         anchor_feature = contrast_feature
# #         anchor_count = contrast_count
# #         # tile mask
# #         mask = mask.repeat(anchor_count, contrast_count)
# #         # mask-out self-contrast cases
# #         logits_mask = torch.scatter(
# #             torch.ones_like(mask),
# #             1,
# #             torch.arange(batch_size).view(-1, 1).to(device),
# #             0
# #         )
# #         # mask = mask * logits_mask
# #
# #         ### 计算同一类图片增强后差值的L2范数
# #         # print(mask[0,:],mask.shape)
# #         # print(mask[1,:],mask.shape)
# #
# #         num = 0
# #         # print(mask[0:2,:5])
# #         mask = mask * logits_mask
# #         index_mask = mask[:, :] == 1
# #         # print(index_mask[0:2,:5])
# #
# #         # sum_loss = torch.Tensor([0]).cuda()
# #         # sum_loss_192 = torch.Tensor([0]).cuda()
# #         sum_loss_92 = torch.Tensor([0]).cuda()
# #         zj_zj = torch.Tensor([0]).cuda()
# #         # div=torch.Tensor([0]).cuda()
# #         # for i in range(len(mask)):
# #         #     for index_feature in contrast_feature[index_mask[i,:]]:
# #         #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
# #         #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
# #         #         sum_loss+=loss_c_192+loss_s_64
# #         #         num+=1
# #         import math
# #
# #         for i in range(len(mask)):
# #
# #             ### 引入 math
# #             zhengjiao_fenzi=contrast_feature[i][:]*contrast_feature[index_mask[i, :]][:,:]
# #             zhengjiao_fenmu=torch.norm(contrast_feature[i][:].unsqueeze(0),dim=1,keepdim=True)*torch.norm(contrast_feature[index_mask[i, :]][:, :],dim=1,keepdim=True)
# #             zhengjiao_fenzi=torch.relu(zhengjiao_fenzi)
# #             zhengjiao_fenmu=torch.relu(zhengjiao_fenmu)
# #
# #             zhengjiao=zhengjiao_fenzi/zhengjiao_fenmu
# #             zj=torch.sum(zhengjiao).cuda()/len(contrast_feature[index_mask[i, :]])
# #
# #
# #             e=torch.tensor(math.e,dtype=torch.float32).cuda()
# #
# #             # print(torch.pow(e,zj)-1)
# #             zj_zj=zj_zj+torch.pow(e,zj)-1
# #         # zj_zj/=mask.shape[0]
# #         print(zj_zj)
# #
# #
# #
# #         # ###
# #         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# #         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
# #         features,features_s_64=torch.split(features,[192,64],dim=1)
# #
# #         labels = labels.contiguous().view(-1, 1)
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output = classifier(features.detach())
# #
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)+zj_zj
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model(images)
# #             features, features_64 = torch.split(features, [192, 64], dim=1)
# #
# #             output = classifier(features)
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #
# #
# #     for ep in range(1000,3250,250):
# #         opt = parse_option()
# #
# #         # build data loader
# #         train_loader, val_loader = set_loader(opt)
# #
# #         # build model and criterion
# #         model, classifier, criterion = set_model(opt)
# #
# #         # build optimizer
# #         optimizer = set_optimizer(opt, classifier)
# #
# #         # training routine
# #         for epoch in range(1, opt.epochs + 1):
# #             adjust_learning_rate(opt, optimizer, epoch)
# #
# #             # train for one epoch
# #             time1 = time.time()
# #             loss, acc = train(train_loader, model, classifier, criterion,
# #                               optimizer, epoch, opt)
# #             time2 = time.time()
# #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #                 epoch, time2 - time1, acc))
# #
# #             # eval for one epoch
# #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #             if val_acc > best_acc:
# #                 best_acc = val_acc
# #
# #         print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
# #
#
#
#
#
#
#
#
#
#
#
#
#
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar10',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_210.pth',
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# #     #                     help='path to pre-trained model')
# #
# #
# #     opt = parser.parse_args()
# #
# #     # set the path according to the environment
# #     opt.data_folder = './datasets/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.encoder.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model.encoder(images)
# #
# #         # ###
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# #         #
# #         #
# #         #
# #         #
# #         #
# #         #
# #         #
# #         # device = (torch.device('cuda')
# #         #           if features.is_cuda
# #         #           else torch.device('cpu'))
# #         #
# #         # batch_size = features.shape[0]/2
# #         # labels = labels.contiguous().view(-1, 1)
# #         # mask = torch.eq(labels, labels.T).float().to(device)
# #         #
# #         #
# #         # # contrast_count = features.shape[1]
# #         # # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# #         #
# #         # # tile mask
# #         # mask = mask.repeat(2,2)
# #         # # mask-out self-contrast cases
# #         # logits_mask = torch.scatter(
# #         #     torch.ones_like(mask),
# #         #     1,
# #         #     torch.arange(batch_size*2).view(-1, 1).to(device).long(),
# #         #     0
# #         # )
# #         # mask = mask * logits_mask
# #         #
# #         #
# #         #
# #         # ### 计算同一类图片增强后差值的L2范数
# #         # # print(mask[0,:],mask.shape)
# #         # # print(mask[1,:],mask.shape)
# #         #
# #         # num = 0
# #         # # print(mask[0:2,:5])
# #         # mask = mask * logits_mask
# #         # index_mask = mask[:, :] == 1
# #         # # print(index_mask[0:2,:5])
# #         #
# #         # # sum_loss = torch.Tensor([0]).cuda()
# #         # # sum_loss_192 = torch.Tensor([0]).cuda()
# #         # sum_loss_64 = torch.Tensor([0]).cuda()
# #         # div=torch.Tensor([0]).cuda()
# #         # # for i in range(len(mask)):
# #         # #     for index_feature in contrast_feature[index_mask[i,:]]:
# #         # #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
# #         # #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
# #         # #         sum_loss+=loss_c_192+loss_s_64
# #         # #         num+=1
# #         # for i in range(len(mask)):
# #         #     # print("******")
# #         #     # print((contrast_feature[i] - contrast_feature[index_mask[i, :]])[:].shape)
# #         #     num += 1
# #         #     # sum_loss_192 += torch.sum(
# #         #     #     torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# #         #     #                    len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# #         #     sum_loss_64 += torch.sum(
# #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# #         #                        len(contrast_feature[index_mask[i, :]]))
# #         #
# #         #     c_192=F.normalize(torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]),dim=0),dim=1)
# #         #     s_64=F.normalize(torch.cat((contrast_feature[i:i+1,192:],contrast_feature[index_mask[i,:]][:,192:]),dim=0).repeat(1,3),dim=1)
# #         #
# #         #     # print(torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1)))
# #         #     div+=torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1))
# #         #     # div+=torch.sum(torch.norm(,dim=0))-,dim=0).repeat(1,3)))/(len(contrast_feature[index_mask[i, :]])+1)
# #         #     # print(div)
# #         #
# #         #     # print(torch.norm(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# #         #     # loss_c_192=torch.sum(torch.norm(contrast_feature[i]-contrast_feature[index_mask[i,:]]))
# #         #     # print(loss_c_192)
# #         # # sum_loss = sum_loss_192 / sum_loss_64
# #         # # sum_loss_192/=num
# #         # # sum_loss_64/=-num
# #         # sum_loss_64=0.001*sum_loss_64
# #         # div/=num
# #         # div*=0.1
# #         # # print(div)
# #         # # print(sum_loss_64.data)
# #         # # sum_loss/=num
# #         # # print("sum_loss:", sum_loss)
# #
# #         # features_c_192, features_s_64= torch.split(contrast_feature, [192, 64], dim=1)
# #
# #
# #
# #         # ### 计算同一张图片增强后差值向量的l2范数
# #         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
# #         # loss_c_192 = torch.sum(torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1)) / len(
# #         #     torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1))
# #         # loss_s_64 = -torch.sum(torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1)) / len(
# #         #     torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1))
# #         # print(loss_c_192, loss_s_64)
# #
# #
# #         # ### 计算同一类图片增强后差值向量的L2范数
# #         #
# #         # contrast_feature=features
# #         # device = (torch.device('cuda')
# #         #           if features.is_cuda
# #         #           else torch.device('cpu'))
# #
# #         labels = labels.contiguous().view(-1, 1)
# #         # mask = torch.eq(labels, labels.T).float().to(device)
# #         # mask = mask.repeat(2, 2)
# #         # num = 0
# #         # index_mask = mask[:, :] == 1
# #         # sum_loss_192 = torch.Tensor([0]).cuda()
# #         # sum_loss_64 = torch.Tensor([0]).cuda()
# #         # for i in range(len(mask)):
# #         #     num += 1
# #         #     sum_loss_192 += torch.sum(
# #         #         torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
# #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# #         #     sum_loss_64 += torch.sum(
# #         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# #         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
# #         # sum_loss_192/=num
# #         # sum_loss_64/=-num
# #         # print(sum_loss_192,sum_loss_64)
# #         # print(contrast_feature.shape)
# #         # features,features_64= torch.split(contrast_feature, [192, 64], dim=1)
# #         # print(features.shape)
# #
# #         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #
# #
# #
# #         output = classifier(features.detach())
# #         # output1 = classifier(features_s_64.detach(),64)
# #
# #
# #
# #
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.encoder.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model.encoder(images)
# #             # features, features_64 = torch.split(features, [192, 64], dim=1)
# #
# #             output = classifier(features)
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #     opt = parse_option()
# #
# #     # build data loader
# #     train_loader, val_loader = set_loader(opt)
# #
# #     # build model and criterion
# #     model, classifier, criterion = set_model(opt)
# #
# #     # build optimizer
# #     optimizer = set_optimizer(opt, classifier)
# #
# #     # training routine
# #     for epoch in range(1, opt.epochs + 1):
# #         adjust_learning_rate(opt, optimizer, epoch)
# #
# #         # train for one epoch
# #         time1 = time.time()
# #         loss, acc = train(train_loader, model, classifier, criterion,
# #                           optimizer, epoch, opt)
# #         time2 = time.time()
# #         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #             epoch, time2 - time1, acc))
# #
# #         # eval for one epoch
# #         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #         if val_acc > best_acc:
# #             best_acc = val_acc
# #
# #     print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar10',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# #     #                     help='path to pre-trained model')
# #
# #
# #     opt = parser.parse_args()
# #
# #     # set the path according to the environment
# #     opt.data_folder = './datasets/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #
# #
# #
# #         device = (torch.device('cuda')
# #                   if features.is_cuda
# #                   else torch.device('cpu'))
# #
# #
# #
# #         batch_size = features.shape[0]
# #
# #         labels = labels.contiguous().view(-1, 1)
# #         if labels.shape[0] != batch_size:
# #             raise ValueError('Num of labels does not match num of features')
# #         mask = torch.eq(labels, labels.T).float().to(device)
# #
# #
# #         contrast_count = features.shape[1]
# #         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
# #
# #         # tile mask
# #         mask = mask.repeat(2, contrast_count)
# #         # mask-out self-contrast cases
# #         logits_mask = torch.scatter(
# #             torch.ones_like(mask),
# #             1,
# #             torch.arange(batch_size * 2).view(-1, 1).to(device),
# #             0
# #         )
# #         mask = mask * logits_mask
# #
# #         ### 计算同一类图片增强后差值的L2范数
# #         # print(mask[0,:],mask.shape)
# #         # print(mask[1,:],mask.shape)
# #
# #         num = 0
# #         mask = mask * logits_mask
# #         index_mask = mask[:, :] == 1
# #         sum_loss_64 = torch.Tensor([0]).cuda()
# #         div = torch.Tensor([0]).cuda()
# #         for i in range(len(mask)):
# #             num += 1
# #             sum_loss_64 += torch.sum(
# #                 torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
# #                                len(contrast_feature[index_mask[i, :]]))
# #
# #             c_192 = F.normalize(
# #                 torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]), dim=0), dim=1)
# #             s_64 = F.normalize(
# #                 torch.cat((contrast_feature[i:i + 1, 192:], contrast_feature[index_mask[i, :]][:, 192:]), dim=0).repeat(
# #                     1, 3), dim=1)
# #
# #             div += torch.sum(torch.norm(c_192 - s_64, dim=1)) / len(torch.norm(c_192 - s_64, dim=1))
# #
# #         sum_loss_64 = -0.001 * sum_loss_64
# #         div /= -num
# #         div *= 0.1
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #
# #         # ###
# #         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# #
# #         # labels = labels.contiguous().view(-1, 1)
# #
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output = classifier(features.detach())
# #
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)
# #         loss=loss+div+sum_loss_64
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model(images)
# #             features, features_64 = torch.split(features, [192, 64], dim=1)
# #
# #             output = classifier(features)
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #
# #
# #     for ep in range(1000,3250,250):
# #         opt = parse_option()
# #
# #         # build data loader
# #         train_loader, val_loader = set_loader(opt)
# #
# #         # build model and criterion
# #         model, classifier, criterion = set_model(opt)
# #
# #         # build optimizer
# #         optimizer = set_optimizer(opt, classifier)
# #
# #         # training routine
# #         for epoch in range(1, opt.epochs + 1):
# #             adjust_learning_rate(opt, optimizer, epoch)
# #
# #             # train for one epoch
# #             time1 = time.time()
# #             loss, acc = train(train_loader, model, classifier, criterion,
# #                               optimizer, epoch, opt)
# #             time2 = time.time()
# #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #                 epoch, time2 - time1, acc))
# #
# #             # eval for one epoch
# #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #             if val_acc > best_acc:
# #                 best_acc = val_acc
# #
# #         print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
# #
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
# #
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar10',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
# #     #                     help='path to pre-trained model')
# #
# #
# #     opt = parser.parse_args()
# #
# #     # set the path according to the environment
# #     opt.data_folder = './datasets/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #
# #         # ###
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# #         features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# #         labels = labels.contiguous().view(-1, 1)
# #
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output = classifier(features.detach(),1)
# #         output2=classifier(features_s_64.detach(),2)
# #
# #         output=output[:,9]+0.1*torch.sum(output2,dim=1).unsqueeze(1)
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)
# #         # loss2=criterion(output2,labels)
# #         # loss=loss-loss2
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model(images)
# #             features, features_64 = torch.split(features, [192, 64], dim=1)
# #
# #             output = classifier(features,1)
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #
# #
# #     for ep in range(1000,3250,250):
# #         opt = parse_option()
# #
# #         # build data loader
# #         train_loader, val_loader = set_loader(opt)
# #
# #         # build model and criterion
# #         model, classifier, criterion = set_model(opt)
# #
# #         # build optimizer
# #         optimizer = set_optimizer(opt, classifier)
# #
# #         # training routine
# #         for epoch in range(1, opt.epochs + 1):
# #             adjust_learning_rate(opt, optimizer, epoch)
# #
# #             # train for one epoch
# #             time1 = time.time()
# #             loss, acc = train(train_loader, model, classifier, criterion,
# #                               optimizer, epoch, opt)
# #             time2 = time.time()
# #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #                 epoch, time2 - time1, acc))
# #
# #             # eval for one epoch
# #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #             if val_acc > best_acc:
# #                 best_acc = val_acc
# #
# #         print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
# #
# #
#
#
#
#
#
#
#
#
#
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# #
# #
# # import torch.nn.functional as F
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,# 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar100',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     # parser.add_argument('--cosine', action='store_true',
# #     #                     help='using cosine annealing')
# #     parser.add_argument('--cosine', default='true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #
# #     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_930.pth', #170
# #                         help='path to pre-trained model')
# #
# #     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
# #     #                     help='path to pre-trained model')
# #
# #     opt = parser.parse_args()
# #     # set the path according to the environment
# #     opt.data_folder = './datasets1/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model = torch.nn.DataParallel(model)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #         # ###
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         features, features_s_64 = torch.split(features, [192, 64], dim=2)
# #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# #
# #         labels = labels.contiguous().view(-1, 1)
# #
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output = classifier(features.detach())
# #
# #
# #         labels=labels.squeeze(1)
# #         loss = criterion(output, labels)
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                    data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #
# #
# #
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features=model(images)
# #             features, features_64 = torch.split(features, [192, 64], dim=1)
# #
# #             output = classifier(features)
# #
# #             # output1=classifier(features_64,64)
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                        idx, len(val_loader), batch_time=batch_time,
# #                        loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #
# #
# #     for ep in range(1000,3250,250):
# #         opt = parse_option()
# #
# #         # build data loader
# #         train_loader, val_loader = set_loader(opt)
# #
# #         # build model and criterion
# #         model, classifier, criterion = set_model(opt)
# #
# #         # build optimizer
# #         optimizer = set_optimizer(opt, classifier)
# #
# #         # training routine
# #         for epoch in range(1, opt.epochs + 1):
# #             adjust_learning_rate(opt, optimizer, epoch)
# #
# #             # train for one epoch
# #             time1 = time.time()
# #             loss, acc = train(train_loader, model, classifier, criterion,
# #                               optimizer, epoch, opt)
# #             time2 = time.time()
# #             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #                 epoch, time2 - time1, acc))
# #
# #             # eval for one epoch
# #             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #             if val_acc > best_acc:
# #                 best_acc = val_acc
# #
# #         print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
#




#
#
# from __future__ import print_function
#
# import sys
# import argparse
# import time
# import math
#
# import torch
# import torch.backends.cudnn as cudnn
#
# from main_ce import set_loader
# from util import AverageMeter
# from util import adjust_learning_rate, warmup_learning_rate, accuracy
# from util import set_optimizer
# from networks.resnet_big import SupConResNet, LinearClassifier
#
# try:
#     import apex
#     from apex import amp, optimizers
# except ImportError:
#     pass
#
#
# def parse_option():
#     parser = argparse.ArgumentParser('argument for training')
#
#     parser.add_argument('--print_freq', type=int, default=10,
#                         help='print frequency')
#     parser.add_argument('--save_freq', type=int, default=50,
#                         help='save frequency')
#     parser.add_argument('--batch_size', type=int, default=256,#256
#                         help='batch_size')
#     parser.add_argument('--num_workers', type=int, default=4, # 16
#                         help='num of workers to use')
#     parser.add_argument('--epochs', type=int, default=100,
#                         help='number of training epochs')
#
#     # optimization
#     parser.add_argument('--learning_rate', type=float, default=0.1,
#                         help='learning rate')
#     parser.add_argument('--lr_decay_epochs', type=str, default='2,4,8',
#                         help='where to decay lr, can be a list')
#     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
#                         help='decay rate for learning rate')
#     parser.add_argument('--weight_decay', type=float, default=0,
#                         help='weight decay')
#     parser.add_argument('--momentum', type=float, default=0.9,
#                         help='momentum')
#
#     # model dataset
#     parser.add_argument('--model', type=str, default='resnet50')
#     parser.add_argument('--dataset', type=str, default='cifar100',
#                         choices=['cifar10', 'cifar100'], help='dataset')
#
#     # other setting
#     parser.add_argument('--cosine', action='store_true',
#                         help='using cosine annealing')
#     parser.add_argument('--warm', action='store_true',
#                         help='warm-up for large batch training')
#
#     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_183.pth', #170
#                         help='path to pre-trained model')
#
#     opt = parser.parse_args()
#
#     # set the path according to the environment
#     opt.data_folder = './datasets1/'
#
#     iterations = opt.lr_decay_epochs.split(',')
#     opt.lr_decay_epochs = list([])
#     for it in iterations:
#         opt.lr_decay_epochs.append(int(it))
#
#     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
#         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
#                opt.batch_size)
#
#     if opt.cosine:
#         opt.model_name = '{}_cosine'.format(opt.model_name)
#
#     # warm-up for large-batch training,
#     if opt.warm:
#         opt.model_name = '{}_warm'.format(opt.model_name)
#         opt.warmup_from = 0.01
#         opt.warm_epochs = 10
#         if opt.cosine:
#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
#         else:
#             opt.warmup_to = opt.learning_rate
#
#     if opt.dataset == 'cifar10':
#         opt.n_cls = 10
#     elif opt.dataset == 'cifar100':
#         opt.n_cls = 100
#     else:
#         raise ValueError('dataset not supported: {}'.format(opt.dataset))
#
#     return opt
#
#
# def set_model(opt):
#     model = SupConResNet(name=opt.model)
#     criterion = torch.nn.CrossEntropyLoss()
#
#     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
#
#     ckpt = torch.load(opt.ckpt, map_location='cpu')
#     state_dict = ckpt['model']
#
#     if torch.cuda.is_available():
#         if torch.cuda.device_count() > 1:
#             model.encoder = torch.nn.DataParallel(model.encoder)
#         else:
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k = k.replace("module.", "")
#                 new_state_dict[k] = v
#             state_dict = new_state_dict
#         model = model.cuda()
#         classifier = classifier.cuda()
#         criterion = criterion.cuda()
#         cudnn.benchmark = True
#
#         model.load_state_dict(state_dict)
#
#     return model, classifier, criterion
#
#
# def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
#     """one epoch training"""
#     model.eval()
#     classifier.train()
#
#     batch_time = AverageMeter()
#     data_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     end = time.time()
#     for idx, (images, labels) in enumerate(train_loader):
#         images = torch.cat([images[0], images[1]], dim=0)
#
#         data_time.update(time.time() - end)
#
#         images = images.cuda(non_blocking=True)
#         labels = labels.cuda(non_blocking=True)
#         bsz = labels.shape[0]
#
#         # warm-up learning rate
#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
#
#         # compute loss
#         with torch.no_grad():
#             features = model(images)
#
#
#         # ###
#         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         features, features_b,_ = torch.split(features, [164,28, 64], dim=2)
#         features = torch.cat(torch.unbind(features, dim=1), dim=0)
#         features_b = torch.cat(torch.unbind(features_b, dim=1), dim=0)
#         # features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
#         # labels = labels.contiguous().view(-1, 1)
#
#         labels = torch.cat((labels, labels), dim=0)
#
#
#         output = classifier(features.detach())
#         output_b=classifier(features_b.detach(),1)
#
#         loss_a = criterion(output, labels)
#         # loss_b=criterion(output_b,labels)
#         loss=loss_a
#         # update metric
#         losses.update(loss.item(), bsz)
#         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#         top1.update(acc1[0], bsz)
#
#         # SGD
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # measure elapsed time
#         batch_time.update(time.time() - end)
#         end = time.time()
#
#         # print info
#         if (idx + 1) % opt.print_freq == 0:
#             print('Train: [{0}][{1}/{2}]\t'
#                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
#                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
#                    data_time=data_time, loss=losses, top1=top1))
#             sys.stdout.flush()
#
#     return losses.avg, top1.avg
#
#
# def validate(val_loader, model, classifier, criterion, opt):
#     """validation"""
#     model.eval()
#     classifier.eval()
#
#     batch_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     with torch.no_grad():
#         end = time.time()
#         for idx, (images, labels) in enumerate(val_loader):
#             images = images.float().cuda()
#             labels = labels.cuda()
#             bsz = labels.shape[0]
#
#             # forward
#             features=model(images)
#             features1, features_b, _ = torch.split(features, [164,28, 64], dim=1)
#             output = classifier(features1.detach())
#             output_b=classifier(features_b.detach(),1)
#
#
#
#
#             loss_a = criterion(output, labels)
#             # loss_b=criterion(output_b,labels)
#             loss=loss_a
#
#             # update metric
#             losses.update(loss.item(), bsz)
#             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#             top1.update(acc1[0], bsz)
#
#             # measure elapsed time
#             batch_time.update(time.time() - end)
#             end = time.time()
#
#             if idx % opt.print_freq == 0:
#                 print('Test: [{0}/{1}]\t'
#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                        idx, len(val_loader), batch_time=batch_time,
#                        loss=losses, top1=top1))
#
#     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
#     return losses.avg, top1.avg
#
#
# def main():
#     best_acc = 0
#     opt = parse_option()
#
#     # build data loader
#     train_loader, val_loader = set_loader(opt)
#
#     # build model and criterion
#     model, classifier, criterion = set_model(opt)
#
#     # build optimizer
#     optimizer = set_optimizer(opt, classifier)
#
#     # training routine
#     for epoch in range(1, opt.epochs + 1):
#         adjust_learning_rate(opt, optimizer, epoch)
#
#         # train for one epoch
#         time1 = time.time()
#         loss, acc = train(train_loader, model, classifier, criterion,
#                           optimizer, epoch, opt)
#         time2 = time.time()
#         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
#             epoch, time2 - time1, acc))
#
#         # eval for one epoch
#         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
#         if val_acc > best_acc:
#             best_acc = val_acc
#
#     print('best accuracy: {:.2f}'.format(best_acc))
#
#
# if __name__ == '__main__':
#     main()







# from __future__ import print_function
#
# import sys
# import argparse
# import time
# import math
#
# import torch
# import torch.backends.cudnn as cudnn
#
# from main_ce import set_loader
# from util import AverageMeter
# from util import adjust_learning_rate, warmup_learning_rate, accuracy
# from util import set_optimizer
# from networks.resnet_big import SupConResNet, LinearClassifier
#
#
#
# import torch.nn.functional as F
#
# try:
#     import apex
#     from apex import amp, optimizers
# except ImportError:
#     pass
#
#
# def parse_option():
#     parser = argparse.ArgumentParser('argument for training')
#
#     parser.add_argument('--print_freq', type=int, default=10,
#                         help='print frequency')
#     parser.add_argument('--save_freq', type=int, default=50,
#                         help='save frequency')
#     parser.add_argument('--batch_size', type=int, default=256,# 256
#                         help='batch_size')
#     parser.add_argument('--num_workers', type=int, default=6,
#                         help='num of workers to use')
#     parser.add_argument('--epochs', type=int, default=100,
#                         help='number of training epochs')
#
#     # optimization
#     parser.add_argument('--learning_rate', type=float, default=0.1,
#                         help='learning rate')
#     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90', # 60,75,90
#                         help='where to decay lr, can be a list')
#     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
#                         help='decay rate for learning rate')
#     parser.add_argument('--weight_decay', type=float, default=0,
#                         help='weight decay')
#     parser.add_argument('--momentum', type=float, default=0.9,
#                         help='momentum')
#
#     # model dataset
#     parser.add_argument('--model', type=str, default='resnet50')
#     parser.add_argument('--dataset', type=str, default='cifar100',
#                         choices=['cifar10', 'cifar100'], help='dataset')
#
#     # other setting
#     # parser.add_argument('--cosine', action='store_true',
#     #                     help='using cosine annealing')
#     parser.add_argument('--cosine', default='true',
#                         help='using cosine annealing')
#     parser.add_argument('--warm', action='store_true',
#                         help='warm-up for large batch training')
#
#
#     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_70.pth', #170
#                         help='path to pre-trained model')
#
#     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
#     #                     help='path to pre-trained model')
#
#     opt = parser.parse_args()
#     # set the path according to the environment
#     opt.data_folder = './datasets1/'
#
#     iterations = opt.lr_decay_epochs.split(',')
#     opt.lr_decay_epochs = list([])
#     for it in iterations:
#         opt.lr_decay_epochs.append(int(it))
#
#     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
#         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
#                opt.batch_size)
#
#     if opt.cosine:
#         opt.model_name = '{}_cosine'.format(opt.model_name)
#
#     # warm-up for large-batch training,
#     if opt.warm:
#         opt.model_name = '{}_warm'.format(opt.model_name)
#         opt.warmup_from = 0.01
#         opt.warm_epochs = 10
#         if opt.cosine:
#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
#         else:
#             opt.warmup_to = opt.learning_rate
#
#     if opt.dataset == 'cifar10':
#         opt.n_cls = 10
#     elif opt.dataset == 'cifar100':
#         opt.n_cls = 100
#     else:
#         raise ValueError('dataset not supported: {}'.format(opt.dataset))
#
#     return opt
#
#
# def set_model(opt):
#     model = SupConResNet(name=opt.model)
#     criterion = torch.nn.CrossEntropyLoss()
#
#     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
#
#     ckpt = torch.load(opt.ckpt, map_location='cpu')
#     state_dict = ckpt['model']
#
#     if torch.cuda.is_available():
#         if torch.cuda.device_count() > 1:
#             model = torch.nn.DataParallel(model)
#         else:
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k = k.replace("module.", "")
#                 new_state_dict[k] = v
#             state_dict = new_state_dict
#         model = model.cuda()
#         classifier = classifier.cuda()
#         criterion = criterion.cuda()
#         cudnn.benchmark = True
#
#         model.load_state_dict(state_dict)
#
#     return model, classifier, criterion
#
#
# def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
#     """one epoch training"""
#     model.eval()
#     classifier.train()
#
#     batch_time = AverageMeter()
#     data_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     end = time.time()
#     for idx, (images, labels) in enumerate(train_loader):
#         images = torch.cat([images[0], images[1]], dim=0)
#
#         data_time.update(time.time() - end)
#
#         images = images.cuda(non_blocking=True)
#         labels = labels.cuda(non_blocking=True)
#         bsz = labels.shape[0]
#
#         # warm-up learning rate
#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
#
#         # compute loss
#         with torch.no_grad():
#             features = model(images)
#
#         # # ###
#         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         # # features_a,_ = torch.split(features, [192,64], dim=2)
#         # # _,features_b = torch.split(features, [164,92], dim=2)
#         # features_a,features_b,features_c=torch.split(features,[164,28,64],dim=2)
#         # features_a = torch.cat(torch.unbind(features_a, dim=1), dim=0)
#         # features_b = torch.cat(torch.unbind(features_b, dim=1), dim=0)
#         # features_c = torch.cat(torch.unbind(features_c, dim=1), dim=0)
#         #
#         # features_n,_=torch.split(features, [192, 64], dim=2)
#         # features_new=torch.cat(torch.unbind(features_n,dim=1),dim=0)
#         #
#         # output_a = classifier(features_a.detach(),1)
#         # output_b = classifier(features_b.detach(),2)
#         # output_c = classifier(features_c.detach(),3)
#         #
#         # import math
#         # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#         # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         # b1 = output_a * output_b / a1
#         # b2 = output_a * output_c / a2
#         # b3 = output_b * output_c / a3
#         # e = torch.tensor(math.e, dtype=torch.float32).cuda()
#         # c1 = torch.pow(e, b1)
#         # c2 = torch.pow(e, b2)
#         # c3 = torch.pow(e, b3)
#         # d = torch.sum(c1+c2+c3)
#         # ###
#         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         # features_a,_ = torch.split(features, [192,64], dim=2)
#         # _,features_b = torch.split(features, [164,92], dim=2)
#         # features_a, _ = torch.split(features, [192, 64], dim=2)
#         # _, features_c = torch.split(features, [192, 64], dim=2)
#         features_a,features_b,features_c=torch.split(features,[164,28,64],dim=2)
#
#
#         features_a = torch.cat(torch.unbind(features_a, dim=1), dim=0)
#         # features_b = torch.cat(torch.unbind(features_b, dim=1), dim=0)
#         features_c = torch.cat(torch.unbind(features_c, dim=1), dim=0)
#
#
#
#         output_a = classifier(features_a.detach(), 1)
#         # output_b = classifier(features_b.detach(), 2)
#         output_c = classifier(features_c.detach(), 3)
#
#
#         features_n, _ = torch.split(features, [192,64], dim=2)
#         # features_n=torch.cat([output_a,output_b,output_c],dim=1)
#         features_new = torch.cat(torch.unbind(features_n, dim=1), dim=0)
#
#
#         ### 特征向量非负
#         output_a = torch.relu(output_a)
#         # output_b=torch.relu(output_b)
#         output_c = torch.relu(output_c)
#
#         import math
#         e = torch.tensor(math.e, dtype=torch.float32).cuda()
#
#         # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#         # b1 = torch.sum(output_a * output_b, dim=1 , keepdim=True) / a1
#         # c1=torch.pow(e,b1)-1
#
#         a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         b2 = torch.sum(output_a * output_c, dim=1,keepdim=True) / a2
#         c2 = torch.pow(e, b2) - 1
#
#         # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#         # b3 = torch.sum(output_b * output_c, dim=1, keepdim=True) / a3
#         # c3 = torch.pow(e, b3) - 1
#
#         # d = torch.sum(c1)+torch.sum(c2)+torch.sum(c3)
#         d=torch.sum(c2)
#
#         labels = labels.contiguous().view(-1, 1)
#
#
#         labels = torch.cat((labels, labels), dim=0)
#
#         output = classifier(features_new.detach())
#
#
#         labels=labels.squeeze(1)
#         loss = criterion(output, labels)+d
#
#         # update metric
#         losses.update(loss.item(), bsz)
#         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#         top1.update(acc1[0], bsz)
#
#         # SGD
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # measure elapsed time
#         batch_time.update(time.time() - end)
#         end = time.time()
#
#         # print info
#         if (idx + 1) % opt.print_freq == 0:
#             print('Train: [{0}][{1}/{2}]\t'
#                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
#                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
#                    data_time=data_time, loss=losses, top1=top1))
#             sys.stdout.flush()
#
#     return losses.avg, top1.avg
#
#
# def validate(val_loader, model, classifier, criterion, opt):
#     """validation"""
#     model.eval()
#     classifier.eval()
#
#     batch_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     with torch.no_grad():
#         end = time.time()
#         for idx, (images, labels) in enumerate(val_loader):
#
#
#
#             images = images.float().cuda()
#             labels = labels.cuda()
#             bsz = labels.shape[0]
#
#             # # forward
#             # features=model(images)
#             # features_a, features_b, features_c = torch.split(features, [164, 28, 64], dim=1)
#             #
#             # output_a = classifier(features_a.detach(), 1)
#             # output_b = classifier(features_b.detach(), 2)
#             # output_c = classifier(features_c.detach(), 3)
#             #
#             # import math
#             # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#             # a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             # b1 = output_a * output_b / a1
#             # b2 = output_a * output_c / a2
#             # b3 = output_b * output_c / a3
#             # e = torch.tensor(math.e, dtype=torch.float32).cuda()
#             # c1 = torch.pow(e, b1)
#             # c2 = torch.pow(e, b2)
#             # c3 = torch.pow(e, b3)
#             # d = torch.sum(c1 + c2 + c3)
#             #
#             # features_n, _ = torch.split(features, [192, 64], dim=1)
#             # output = classifier(features_n.detach())
#
#             # forward
#             features = model(images)
#             features_a, features_b, features_c = torch.split(features, [164, 28, 64], dim=1)
#
#
#             output_a = classifier(features_a.detach(), 1)
#             # output_b = classifier(features_b.detach(), 2)
#             output_c = classifier(features_c.detach(), 3)
#
#             features_new, _ = torch.split(features, [192,64], dim=1)
#             # features_n = torch.cat([output_a, output_b, output_c], dim=1)
#             # features_new = torch.cat(torch.unbind(features_n, dim=1), dim=0)
#
#             ### 特征向量非负
#             output_a = torch.relu(output_a)
#             # output_b=torch.relu(output_b)
#             output_c = torch.relu(output_c)
#
#             import math
#             e = torch.tensor(math.e, dtype=torch.float32).cuda()
#
#             # a1 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_b, dim=1, keepdim=True)
#             # b1 = torch.sum(output_a * output_b, dim=1, keepdim=True) / a1
#             # c1 = torch.pow(e, b1) - 1
#
#             a2 = torch.norm(output_a, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             b2 = torch.sum(output_a * output_c, dim=1, keepdim=True) / a2
#             c2 = torch.pow(e, b2) - 1
#
#             # a3 = torch.norm(output_b, dim=1, keepdim=True) * torch.norm(output_c, dim=1, keepdim=True)
#             # b3 = torch.sum(output_b * output_c, dim=1, keepdim=True) / a3
#             # c3 = torch.pow(e, b3) - 1
#             # d = torch.sum(c1 + c2 + c3)
#             # d=torch.sum(c1)+torch.sum(c2)+torch.sum(c3)
#             d=torch.sum(c2)
#             output = classifier(features_new.detach())
#
#
#             # output1=classifier(features_64,64)
#
#             loss = criterion(output, labels)+d
#
#             # update metric
#             losses.update(loss.item(), bsz)
#             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#             top1.update(acc1[0], bsz)
#
#             # measure elapsed time
#             batch_time.update(time.time() - end)
#             end = time.time()
#
#             if idx % opt.print_freq == 0:
#                 print('Test: [{0}/{1}]\t'
#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                        idx, len(val_loader), batch_time=batch_time,
#                        loss=losses, top1=top1))
#
#     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
#     return losses.avg, top1.avg
#
#
# def main():
#     best_acc = 0
#
#
#     for ep in range(1000,3250,250):
#         opt = parse_option()
#
#         # build data loader
#         train_loader, val_loader = set_loader(opt)
#
#         # build model and criterion
#         model, classifier, criterion = set_model(opt)
#
#         # build optimizer
#         optimizer = set_optimizer(opt, classifier)
#
#         # training routine
#         for epoch in range(1, opt.epochs + 1):
#             adjust_learning_rate(opt, optimizer, epoch)
#
#             # train for one epoch
#             time1 = time.time()
#             loss, acc = train(train_loader, model, classifier, criterion,
#                               optimizer, epoch, opt)
#             time2 = time.time()
#             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
#                 epoch, time2 - time1, acc))
#
#             # eval for one epoch
#             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
#             if val_acc > best_acc:
#                 best_acc = val_acc
#
#         print('best accuracy: {:.2f}'.format(best_acc))
#
#
# if __name__ == '__main__':
#     main()




















from __future__ import print_function

import sys
import argparse
import time
import math

import torch
import torch.backends.cudnn as cudnn

from main_ce import set_loader
from util import AverageMeter
from util import adjust_learning_rate, warmup_learning_rate, accuracy
from util import set_optimizer
from networks.resnet_big import SupConResNet, LinearClassifier



import torch.nn.functional as F

try:
    import apex
    from apex import amp, optimizers
except ImportError:
    pass


def parse_option():
    parser = argparse.ArgumentParser('argument for training')

    parser.add_argument('--print_freq', type=int, default=10,
                        help='print frequency')
    parser.add_argument('--save_freq', type=int, default=50,
                        help='save frequency')
    parser.add_argument('--batch_size', type=int, default=256,# 256
                        help='batch_size')
    parser.add_argument('--num_workers', type=int, default=32,
                        help='num of workers to use')
    parser.add_argument('--epochs', type=int, default=100,
                        help='number of training epochs')

    # optimization
    parser.add_argument('--learning_rate', type=float, default=0.1,
                        help='learning rate')
    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90', # 60,75,90
                        help='where to decay lr, can be a list')
    parser.add_argument('--lr_decay_rate', type=float, default=0.2,
                        help='decay rate for learning rate')
    parser.add_argument('--weight_decay', type=float, default=0,
                        help='weight decay')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='momentum')

    # model dataset
    parser.add_argument('--model', type=str, default='resnet50')
    parser.add_argument('--dataset', type=str, default='cifar100',
                        choices=['cifar10', 'cifar100'], help='dataset')

    # other setting
    # parser.add_argument('--cosine', action='store_true',
    #                     help='using cosine annealing')
    parser.add_argument('--cosine', default='true',
                        help='using cosine annealing')
    parser.add_argument('--warm', action='store_true',
                        help='warm-up for large batch training')


    parser.add_argument('--ckpt', type=str, default='/home/wangbin/Desktop/SupContrast-master(1)/save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/ckpt_epoch_100.pth', #170
                        help='path to pre-trained model')

    # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
    #                     help='path to pre-trained model')

    opt = parser.parse_args()
    # set the path according to the environment
    opt.data_folder = './datasets1/'

    iterations = opt.lr_decay_epochs.split(',')
    opt.lr_decay_epochs = list([])
    for it in iterations:
        opt.lr_decay_epochs.append(int(it))

    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
               opt.batch_size)

    if opt.cosine:
        opt.model_name = '{}_cosine'.format(opt.model_name)

    # warm-up for large-batch training,
    if opt.warm:
        opt.model_name = '{}_warm'.format(opt.model_name)
        opt.warmup_from = 0.01
        opt.warm_epochs = 10
        if opt.cosine:
            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
        else:
            opt.warmup_to = opt.learning_rate

    if opt.dataset == 'cifar10':
        opt.n_cls = 10
    elif opt.dataset == 'cifar100':
        opt.n_cls = 100
    else:
        raise ValueError('dataset not supported: {}'.format(opt.dataset))

    return opt


def set_model(opt):
    model = SupConResNet(name=opt.model)
    criterion = torch.nn.CrossEntropyLoss()

    classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)

    ckpt = torch.load(opt.ckpt, map_location='cpu')
    state_dict = ckpt['model']

    if torch.cuda.is_available():
        if torch.cuda.device_count() > 1:
            model = torch.nn.DataParallel(model)
        else:
            new_state_dict = {}
            for k, v in state_dict.items():
                k = k.replace("module.", "")
                new_state_dict[k] = v
            state_dict = new_state_dict
        model = model.cuda()
        classifier = classifier.cuda()
        criterion = criterion.cuda()
        cudnn.benchmark = True

        model.load_state_dict(state_dict)

    return model, classifier, criterion


def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
    """one epoch training"""
    model.eval()
    classifier.train()

    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    end = time.time()
    for idx, (images, labels) in enumerate(train_loader):
        images = torch.cat([images[0], images[1]], dim=0)

        data_time.update(time.time() - end)

        images = images.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)
        bsz = labels.shape[0]

        # warm-up learning rate
        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)

        # compute loss
        with torch.no_grad():
            features,_ = model(images)

        # ###
        f1, f2 = torch.split(features, [bsz, bsz], dim=0)
        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
        features, features_s_64 = torch.split(features, [192, 64], dim=2)
        features = torch.cat(torch.unbind(features, dim=1), dim=0)

        labels = labels.contiguous().view(-1, 1)


        labels = torch.cat((labels, labels), dim=0)

        output = classifier(features.detach())


        labels=labels.squeeze(1)
        loss = criterion(output, labels)

        # update metric
        losses.update(loss.item(), bsz)
        acc1, acc5 = accuracy(output, labels, topk=(1, 5))
        top1.update(acc1[0], bsz)

        # SGD
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        # print info
        if (idx + 1) % opt.print_freq == 0:
            print('Train: [{0}][{1}/{2}]\t'
                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
                  'loss {loss.val:.3f} ({loss.avg:.3f})\t'
                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
                   epoch, idx + 1, len(train_loader), batch_time=batch_time,
                   data_time=data_time, loss=losses, top1=top1))
            sys.stdout.flush()

    return losses.avg, top1.avg


def validate(val_loader, model, classifier, criterion, opt):
    """validation"""
    model.eval()
    classifier.eval()

    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    with torch.no_grad():
        end = time.time()
        for idx, (images, labels) in enumerate(val_loader):



            images = images.float().cuda()
            labels = labels.cuda()
            bsz = labels.shape[0]

            # forward
            features,_=model(images)
            features, features_64 = torch.split(features, [192, 64], dim=1)

            output = classifier(features)

            # output1=classifier(features_64,64)

            loss = criterion(output, labels)

            # update metric
            losses.update(loss.item(), bsz)
            acc1, acc5 = accuracy(output, labels, topk=(1, 5))
            top1.update(acc1[0], bsz)

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if idx % opt.print_freq == 0:
                print('Test: [{0}/{1}]\t'
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
                       idx, len(val_loader), batch_time=batch_time,
                       loss=losses, top1=top1))

    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
    return losses.avg, top1.avg


def main():
    best_acc = 0


    for ep in range(1000,3250,250):
        opt = parse_option()

        # build data loader
        train_loader, val_loader = set_loader(opt)

        # build model and criterion
        model, classifier, criterion = set_model(opt)

        # build optimizer
        optimizer = set_optimizer(opt, classifier)

        # training routine
        for epoch in range(1, opt.epochs + 1):
            adjust_learning_rate(opt, optimizer, epoch)

            # train for one epoch
            time1 = time.time()
            loss, acc = train(train_loader, model, classifier, criterion,
                              optimizer, epoch, opt)
            time2 = time.time()
            print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
                epoch, time2 - time1, acc))

            # eval for one epoch
            loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
            if val_acc > best_acc:
                best_acc = val_acc

        print('best accuracy: {:.2f}'.format(best_acc))
        break


if __name__ == '__main__':
    main()
#
# #
# # from __future__ import print_function
# #
# # import sys
# # import argparse
# # import time
# # import math
# #
# # import torch
# # import torch.backends.cudnn as cudnn
# #
# # from main_ce import set_loader
# # from util import AverageMeter
# # from util import adjust_learning_rate, warmup_learning_rate, accuracy
# # from util import set_optimizer
# # from networks.resnet_big import SupConResNet, LinearClassifier
# #
# # try:
# #     import apex
# #     from apex import amp, optimizers
# # except ImportError:
# #     pass
# #
# #
# # def parse_option():
# #     parser = argparse.ArgumentParser('argument for training')
# #
# #     parser.add_argument('--print_freq', type=int, default=10,
# #                         help='print frequency')
# #     parser.add_argument('--save_freq', type=int, default=50,
# #                         help='save frequency')
# #     parser.add_argument('--batch_size', type=int, default=256,  # 256
# #                         help='batch_size')
# #     parser.add_argument('--num_workers', type=int, default=4,  # 16
# #                         help='num of workers to use')
# #     parser.add_argument('--epochs', type=int, default=100,
# #                         help='number of training epochs')
# #
# #     # optimization
# #     parser.add_argument('--learning_rate', type=float, default=0.1,
# #                         help='learning rate')
# #     parser.add_argument('--lr_decay_epochs', type=str, default='2,4,8',
# #                         help='where to decay lr, can be a list')
# #     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
# #                         help='decay rate for learning rate')
# #     parser.add_argument('--weight_decay', type=float, default=0,
# #                         help='weight decay')
# #     parser.add_argument('--momentum', type=float, default=0.9,
# #                         help='momentum')
# #
# #     # model dataset
# #     parser.add_argument('--model', type=str, default='resnet50')
# #     parser.add_argument('--dataset', type=str, default='cifar100',
# #                         choices=['cifar10', 'cifar100'], help='dataset')
# #
# #     # other setting
# #     parser.add_argument('--cosine', action='store_true',
# #                         help='using cosine annealing')
# #     parser.add_argument('--warm', action='store_true',
# #                         help='warm-up for large batch training')
# #
# #     parser.add_argument('--ckpt', type=str,
# #                         default='.\save\SupCon\cifar100_models\SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_50.pth',
# #                         # 170
# #                         help='path to pre-trained model')
# #
# #     opt = parser.parse_args()
# #
# #     # set the path according to the environment
# #     opt.data_folder = './datasets1/'
# #
# #     iterations = opt.lr_decay_epochs.split(',')
# #     opt.lr_decay_epochs = list([])
# #     for it in iterations:
# #         opt.lr_decay_epochs.append(int(it))
# #
# #     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'. \
# #         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
# #                opt.batch_size)
# #
# #     if opt.cosine:
# #         opt.model_name = '{}_cosine'.format(opt.model_name)
# #
# #     # warm-up for large-batch training,
# #     if opt.warm:
# #         opt.model_name = '{}_warm'.format(opt.model_name)
# #         opt.warmup_from = 0.01
# #         opt.warm_epochs = 10
# #         if opt.cosine:
# #             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
# #             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
# #                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
# #         else:
# #             opt.warmup_to = opt.learning_rate
# #
# #     if opt.dataset == 'cifar10':
# #         opt.n_cls = 10
# #     elif opt.dataset == 'cifar100':
# #         opt.n_cls = 100
# #     else:
# #         raise ValueError('dataset not supported: {}'.format(opt.dataset))
# #
# #     return opt
# #
# #
# # def set_model(opt):
# #     model = SupConResNet(name=opt.model)
# #     criterion = torch.nn.CrossEntropyLoss()
# #
# #     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
# #
# #     ckpt = torch.load(opt.ckpt, map_location='cpu')
# #     state_dict = ckpt['model']
# #
# #     if torch.cuda.is_available():
# #         if torch.cuda.device_count() > 1:
# #             model.encoder = torch.nn.DataParallel(model.encoder)
# #         else:
# #             new_state_dict = {}
# #             for k, v in state_dict.items():
# #                 k = k.replace("module.", "")
# #                 new_state_dict[k] = v
# #             state_dict = new_state_dict
# #         model = model.cuda()
# #         classifier = classifier.cuda()
# #         criterion = criterion.cuda()
# #         cudnn.benchmark = True
# #
# #         model.load_state_dict(state_dict)
# #
# #     return model, classifier, criterion
# #
# #
# # def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
# #     """one epoch training"""
# #     model.eval()
# #     classifier.train()
# #
# #     batch_time = AverageMeter()
# #     data_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     end = time.time()
# #     for idx, (images, labels) in enumerate(train_loader):
# #         images = torch.cat([images[0], images[1]], dim=0)
# #
# #         data_time.update(time.time() - end)
# #
# #         images = images.cuda(non_blocking=True)
# #         labels = labels.cuda(non_blocking=True)
# #         bsz = labels.shape[0]
# #
# #         # warm-up learning rate
# #         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
# #
# #         # compute loss
# #         with torch.no_grad():
# #             features = model(images)
# #
# #         # ###
# #         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
# #         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
# #         _, features,_ = torch.split(features, [164,28, 64], dim=2)
# #         features = torch.cat(torch.unbind(features, dim=1), dim=0)
# #         # features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
# #         # labels = labels.contiguous().view(-1, 1)
# #
# #         labels = torch.cat((labels, labels), dim=0)
# #
# #         output = classifier(features.detach())
# #         loss = criterion(output, labels)
# #
# #         # update metric
# #         losses.update(loss.item(), bsz)
# #         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #         top1.update(acc1[0], bsz)
# #
# #         # SGD
# #         optimizer.zero_grad()
# #         loss.backward()
# #         optimizer.step()
# #
# #         # measure elapsed time
# #         batch_time.update(time.time() - end)
# #         end = time.time()
# #
# #         # print info
# #         if (idx + 1) % opt.print_freq == 0:
# #             print('Train: [{0}][{1}/{2}]\t'
# #                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
# #                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
# #                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                 epoch, idx + 1, len(train_loader), batch_time=batch_time,
# #                 data_time=data_time, loss=losses, top1=top1))
# #             sys.stdout.flush()
# #
# #     return losses.avg, top1.avg
# #
# #
# # def validate(val_loader, model, classifier, criterion, opt):
# #     """validation"""
# #     model.eval()
# #     classifier.eval()
# #
# #     batch_time = AverageMeter()
# #     losses = AverageMeter()
# #     top1 = AverageMeter()
# #
# #     with torch.no_grad():
# #         end = time.time()
# #         for idx, (images, labels) in enumerate(val_loader):
# #             images = images.float().cuda()
# #             labels = labels.cuda()
# #             bsz = labels.shape[0]
# #
# #             # forward
# #             features = model(images)
# #             _,features1,_ = torch.split(features, [164,28, 64], dim=1)
# #             output = classifier(features1.detach())
# #
# #             loss = criterion(output, labels)
# #
# #             # update metric
# #             losses.update(loss.item(), bsz)
# #             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
# #             top1.update(acc1[0], bsz)
# #
# #             # measure elapsed time
# #             batch_time.update(time.time() - end)
# #             end = time.time()
# #
# #             if idx % opt.print_freq == 0:
# #                 print('Test: [{0}/{1}]\t'
# #                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
# #                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
# #                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
# #                     idx, len(val_loader), batch_time=batch_time,
# #                     loss=losses, top1=top1))
# #
# #     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
# #     return losses.avg, top1.avg
# #
# #
# # def main():
# #     best_acc = 0
# #     opt = parse_option()
# #
# #     # build data loader
# #     train_loader, val_loader = set_loader(opt)
# #
# #     # build model and criterion
# #     model, classifier, criterion = set_model(opt)
# #
# #     # build optimizer
# #     optimizer = set_optimizer(opt, classifier)
# #
# #     # training routine
# #     for epoch in range(1, opt.epochs + 1):
# #         adjust_learning_rate(opt, optimizer, epoch)
# #
# #         # train for one epoch
# #         time1 = time.time()
# #         loss, acc = train(train_loader, model, classifier, criterion,
# #                           optimizer, epoch, opt)
# #         time2 = time.time()
# #         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
# #             epoch, time2 - time1, acc))
# #
# #         # eval for one epoch
# #         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
# #         if val_acc > best_acc:
# #             best_acc = val_acc
# #
# #     print('best accuracy: {:.2f}'.format(best_acc))
# #
# #
# # if __name__ == '__main__':
# #     main()
# #













































# from __future__ import print_function
#
# import sys
# import argparse
# import time
# import math
#
# import torch
# import torch.backends.cudnn as cudnn
#
# from main_ce import set_loader
# from util import AverageMeter
# from util import adjust_learning_rate, warmup_learning_rate, accuracy
# from util import set_optimizer
# from networks.resnet_big import SupConResNet, LinearClassifier
#
#
#
# import torch.nn.functional as F
#
# try:
#     import apex
#     from apex import amp, optimizers
# except ImportError:
#     pass
#
#
# def parse_option():
#     parser = argparse.ArgumentParser('argument for training')
#
#     parser.add_argument('--print_freq', type=int, default=10,
#                         help='print frequency')
#     parser.add_argument('--save_freq', type=int, default=50,
#                         help='save frequency')
#     parser.add_argument('--batch_size', type=int, default=256,# 256
#                         help='batch_size')
#     parser.add_argument('--num_workers', type=int, default=4,
#                         help='num of workers to use')
#     parser.add_argument('--epochs', type=int, default=100,
#                         help='number of training epochs')
#
#     # optimization
#     parser.add_argument('--learning_rate', type=float, default=0.1,
#                         help='learning rate')
#     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
#                         help='where to decay lr, can be a list')
#     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
#                         help='decay rate for learning rate')
#     parser.add_argument('--weight_decay', type=float, default=0,
#                         help='weight decay')
#     parser.add_argument('--momentum', type=float, default=0.9,
#                         help='momentum')
#
#     # model dataset
#     parser.add_argument('--model', type=str, default='resnet50')
#     parser.add_argument('--dataset', type=str, default='cifar10',
#                         choices=['cifar10', 'cifar100'], help='dataset')
#
#     # other setting
#     # parser.add_argument('--cosine', action='store_true',
#     #                     help='using cosine annealing')
#     parser.add_argument('--cosine', default='true',
#                         help='using cosine annealing')
#     parser.add_argument('--warm', action='store_true',
#                         help='warm-up for large batch training')
#
#     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_210.pth',
#                         help='path to pre-trained model')
#
#     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
#     #                     help='path to pre-trained model')
#
#
#     opt = parser.parse_args()
#
#     # set the path according to the environment
#     opt.data_folder = './datasets/'
#
#     iterations = opt.lr_decay_epochs.split(',')
#     opt.lr_decay_epochs = list([])
#     for it in iterations:
#         opt.lr_decay_epochs.append(int(it))
#
#     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
#         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
#                opt.batch_size)
#
#     if opt.cosine:
#         opt.model_name = '{}_cosine'.format(opt.model_name)
#
#     # warm-up for large-batch training,
#     if opt.warm:
#         opt.model_name = '{}_warm'.format(opt.model_name)
#         opt.warmup_from = 0.01
#         opt.warm_epochs = 10
#         if opt.cosine:
#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
#         else:
#             opt.warmup_to = opt.learning_rate
#
#     if opt.dataset == 'cifar10':
#         opt.n_cls = 10
#     elif opt.dataset == 'cifar100':
#         opt.n_cls = 100
#     else:
#         raise ValueError('dataset not supported: {}'.format(opt.dataset))
#
#     return opt
#
#
# def set_model(opt):
#     model = SupConResNet(name=opt.model)
#     criterion = torch.nn.CrossEntropyLoss()
#
#     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
#
#     ckpt = torch.load(opt.ckpt, map_location='cpu')
#     state_dict = ckpt['model']
#
#     if torch.cuda.is_available():
#         if torch.cuda.device_count() > 1:
#             model = torch.nn.DataParallel(model)
#         else:
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k = k.replace("module.", "")
#                 new_state_dict[k] = v
#             state_dict = new_state_dict
#         model = model.cuda()
#         classifier = classifier.cuda()
#         criterion = criterion.cuda()
#         cudnn.benchmark = True
#
#         model.load_state_dict(state_dict)
#
#     return model, classifier, criterion
#
#
# def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
#     """one epoch training"""
#     model.encoder.eval()
#     classifier.train()
#
#     batch_time = AverageMeter()
#     data_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     end = time.time()
#     for idx, (images, labels) in enumerate(train_loader):
#         images = torch.cat([images[0], images[1]], dim=0)
#
#         data_time.update(time.time() - end)
#
#         images = images.cuda(non_blocking=True)
#         labels = labels.cuda(non_blocking=True)
#         bsz = labels.shape[0]
#
#         # warm-up learning rate
#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
#
#         # compute loss
#         with torch.no_grad():
#             features = model.encoder(images)
#
#         # ###
#         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
#         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
#         #
#         #
#         #
#         #
#         #
#         #
#         #
#         # device = (torch.device('cuda')
#         #           if features.is_cuda
#         #           else torch.device('cpu'))
#         #
#         # batch_size = features.shape[0]/2
#         # labels = labels.contiguous().view(-1, 1)
#         # mask = torch.eq(labels, labels.T).float().to(device)
#         #
#         #
#         # # contrast_count = features.shape[1]
#         # # contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
#         #
#         # # tile mask
#         # mask = mask.repeat(2,2)
#         # # mask-out self-contrast cases
#         # logits_mask = torch.scatter(
#         #     torch.ones_like(mask),
#         #     1,
#         #     torch.arange(batch_size*2).view(-1, 1).to(device).long(),
#         #     0
#         # )
#         # mask = mask * logits_mask
#         #
#         #
#         #
#         # ### 计算同一类图片增强后差值的L2范数
#         # # print(mask[0,:],mask.shape)
#         # # print(mask[1,:],mask.shape)
#         #
#         # num = 0
#         # # print(mask[0:2,:5])
#         # mask = mask * logits_mask
#         # index_mask = mask[:, :] == 1
#         # # print(index_mask[0:2,:5])
#         #
#         # # sum_loss = torch.Tensor([0]).cuda()
#         # # sum_loss_192 = torch.Tensor([0]).cuda()
#         # sum_loss_64 = torch.Tensor([0]).cuda()
#         # div=torch.Tensor([0]).cuda()
#         # # for i in range(len(mask)):
#         # #     for index_feature in contrast_feature[index_mask[i,:]]:
#         # #         loss_c_192=torch.sum(torch.norm(contrast_feature[i][:192]-index_feature[:192]))/192
#         # #         loss_s_64=1-torch.sum(torch.norm(contrast_feature[i][192:]-index_feature[192:]))/64
#         # #         sum_loss+=loss_c_192+loss_s_64
#         # #         num+=1
#         # for i in range(len(mask)):
#         #     # print("******")
#         #     # print((contrast_feature[i] - contrast_feature[index_mask[i, :]])[:].shape)
#         #     num += 1
#         #     # sum_loss_192 += torch.sum(
#         #     #     torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
#         #     #                    len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
#         #     sum_loss_64 += torch.sum(
#         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
#         #                        len(contrast_feature[index_mask[i, :]]))
#         #
#         #     c_192=F.normalize(torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]),dim=0),dim=1)
#         #     s_64=F.normalize(torch.cat((contrast_feature[i:i+1,192:],contrast_feature[index_mask[i,:]][:,192:]),dim=0).repeat(1,3),dim=1)
#         #
#         #     # print(torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1)))
#         #     div+=torch.sum(torch.norm(c_192-s_64,dim=1))/len(torch.norm(c_192-s_64,dim=1))
#         #     # div+=torch.sum(torch.norm(,dim=0))-,dim=0).repeat(1,3)))/(len(contrast_feature[index_mask[i, :]])+1)
#         #     # print(div)
#         #
#         #     # print(torch.norm(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
#         #     # loss_c_192=torch.sum(torch.norm(contrast_feature[i]-contrast_feature[index_mask[i,:]]))
#         #     # print(loss_c_192)
#         # # sum_loss = sum_loss_192 / sum_loss_64
#         # # sum_loss_192/=num
#         # # sum_loss_64/=-num
#         # sum_loss_64=0.001*sum_loss_64
#         # div/=num
#         # div*=0.1
#         # # print(div)
#         # # print(sum_loss_64.data)
#         # # sum_loss/=num
#         # # print("sum_loss:", sum_loss)
#
#         # features_c_192, features_s_64= torch.split(contrast_feature, [192, 64], dim=1)
#
#
#
#         # ### 计算同一张图片增强后差值向量的l2范数
#         # features, features_s_64 = torch.split(features, [192, 64], dim=2)
#         # loss_c_192 = torch.sum(torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1)) / len(
#         #     torch.norm(features_c_192[:, 0, :] - features_c_192[:, 1, :], dim=1))
#         # loss_s_64 = -torch.sum(torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1)) / len(
#         #     torch.norm(features_s_64[:, 0, :] - features_s_64[:, 1, :], dim=1))
#         # print(loss_c_192, loss_s_64)
#
#
#         # ### 计算同一类图片增强后差值向量的L2范数
#         #
#         # contrast_feature=features
#         # device = (torch.device('cuda')
#         #           if features.is_cuda
#         #           else torch.device('cpu'))
#
#         labels = labels.contiguous().view(-1, 1)
#         # mask = torch.eq(labels, labels.T).float().to(device)
#         # mask = mask.repeat(2, 2)
#         # num = 0
#         # index_mask = mask[:, :] == 1
#         # sum_loss_192 = torch.Tensor([0]).cuda()
#         # sum_loss_64 = torch.Tensor([0]).cuda()
#         # for i in range(len(mask)):
#         #     num += 1
#         #     sum_loss_192 += torch.sum(
#         #         torch.norm((contrast_feature[i][:192] - contrast_feature[index_mask[i, :]][:, :192]), dim=1)) / (
#         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
#         #     sum_loss_64 += torch.sum(
#         #         torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
#         #                        len(contrast_feature[i] - contrast_feature[index_mask[i, :]]))
#         # sum_loss_192/=num
#         # sum_loss_64/=-num
#         # print(sum_loss_192,sum_loss_64)
#         # print(contrast_feature.shape)
#         # features,features_64= torch.split(contrast_feature, [192, 64], dim=1)
#         # print(features.shape)
#
#         # features = torch.cat(torch.unbind(features, dim=1), dim=0)
#
#         labels = torch.cat((labels, labels), dim=0)
#
#
#
#
#         output = classifier(features.detach())
#         # output1 = classifier(features_s_64.detach(),64)
#
#
#
#
#
#         labels=labels.squeeze(1)
#         loss = criterion(output, labels)
#
#         # update metric
#         losses.update(loss.item(), bsz)
#         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#         top1.update(acc1[0], bsz)
#
#         # SGD
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # measure elapsed time
#         batch_time.update(time.time() - end)
#         end = time.time()
#
#         # print info
#         if (idx + 1) % opt.print_freq == 0:
#             print('Train: [{0}][{1}/{2}]\t'
#                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
#                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
#                    data_time=data_time, loss=losses, top1=top1))
#             sys.stdout.flush()
#
#     return losses.avg, top1.avg
#
#
# def validate(val_loader, model, classifier, criterion, opt):
#     """validation"""
#     model.encoder.eval()
#     classifier.eval()
#
#     batch_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     with torch.no_grad():
#         end = time.time()
#         for idx, (images, labels) in enumerate(val_loader):
#
#
#
#             images = images.float().cuda()
#             labels = labels.cuda()
#             bsz = labels.shape[0]
#
#             # forward
#             features=model.encoder(images)
#             # features, features_64 = torch.split(features, [192, 64], dim=1)
#
#             output = classifier(features)
#
#             # output1=classifier(features_64,64)
#
#             loss = criterion(output, labels)
#
#             # update metric
#             losses.update(loss.item(), bsz)
#             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#             top1.update(acc1[0], bsz)
#
#             # measure elapsed time
#             batch_time.update(time.time() - end)
#             end = time.time()
#
#             if idx % opt.print_freq == 0:
#                 print('Test: [{0}/{1}]\t'
#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                        idx, len(val_loader), batch_time=batch_time,
#                        loss=losses, top1=top1))
#
#     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
#     return losses.avg, top1.avg
#
#
# def main():
#     best_acc = 0
#     opt = parse_option()
#
#     # build data loader
#     train_loader, val_loader = set_loader(opt)
#
#     # build model and criterion
#     model, classifier, criterion = set_model(opt)
#
#     # build optimizer
#     optimizer = set_optimizer(opt, classifier)
#
#     # training routine
#     for epoch in range(1, opt.epochs + 1):
#         adjust_learning_rate(opt, optimizer, epoch)
#
#         # train for one epoch
#         time1 = time.time()
#         loss, acc = train(train_loader, model, classifier, criterion,
#                           optimizer, epoch, opt)
#         time2 = time.time()
#         print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
#             epoch, time2 - time1, acc))
#
#         # eval for one epoch
#         loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
#         if val_acc > best_acc:
#             best_acc = val_acc
#
#     print('best accuracy: {:.2f}'.format(best_acc))
#
#
# if __name__ == '__main__':
#     main()















#
# from __future__ import print_function
#
# import sys
# import argparse
# import time
# import math
#
# import torch
# import torch.backends.cudnn as cudnn
#
# from main_ce import set_loader
# from util import AverageMeter
# from util import adjust_learning_rate, warmup_learning_rate, accuracy
# from util import set_optimizer
# from networks.resnet_big import SupConResNet, LinearClassifier
#
#
#
# import torch.nn.functional as F
#
# try:
#     import apex
#     from apex import amp, optimizers
# except ImportError:
#     pass
#
#
# def parse_option():
#     parser = argparse.ArgumentParser('argument for training')
#
#     parser.add_argument('--print_freq', type=int, default=10,
#                         help='print frequency')
#     parser.add_argument('--save_freq', type=int, default=50,
#                         help='save frequency')
#     parser.add_argument('--batch_size', type=int, default=256,# 256
#                         help='batch_size')
#     parser.add_argument('--num_workers', type=int, default=4,
#                         help='num of workers to use')
#     parser.add_argument('--epochs', type=int, default=100,
#                         help='number of training epochs')
#
#     # optimization
#     parser.add_argument('--learning_rate', type=float, default=0.1,
#                         help='learning rate')
#     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
#                         help='where to decay lr, can be a list')
#     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
#                         help='decay rate for learning rate')
#     parser.add_argument('--weight_decay', type=float, default=0,
#                         help='weight decay')
#     parser.add_argument('--momentum', type=float, default=0.9,
#                         help='momentum')
#
#     # model dataset
#     parser.add_argument('--model', type=str, default='resnet50')
#     parser.add_argument('--dataset', type=str, default='cifar10',
#                         choices=['cifar10', 'cifar100'], help='dataset')
#
#     # other setting
#     # parser.add_argument('--cosine', action='store_true',
#     #                     help='using cosine annealing')
#     parser.add_argument('--cosine', default='true',
#                         help='using cosine annealing')
#     parser.add_argument('--warm', action='store_true',
#                         help='warm-up for large batch training')
#
#     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
#                         help='path to pre-trained model')
#
#     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
#     #                     help='path to pre-trained model')
#
#
#     opt = parser.parse_args()
#
#     # set the path according to the environment
#     opt.data_folder = './datasets/'
#
#     iterations = opt.lr_decay_epochs.split(',')
#     opt.lr_decay_epochs = list([])
#     for it in iterations:
#         opt.lr_decay_epochs.append(int(it))
#
#     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
#         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
#                opt.batch_size)
#
#     if opt.cosine:
#         opt.model_name = '{}_cosine'.format(opt.model_name)
#
#     # warm-up for large-batch training,
#     if opt.warm:
#         opt.model_name = '{}_warm'.format(opt.model_name)
#         opt.warmup_from = 0.01
#         opt.warm_epochs = 10
#         if opt.cosine:
#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
#         else:
#             opt.warmup_to = opt.learning_rate
#
#     if opt.dataset == 'cifar10':
#         opt.n_cls = 10
#     elif opt.dataset == 'cifar100':
#         opt.n_cls = 100
#     else:
#         raise ValueError('dataset not supported: {}'.format(opt.dataset))
#
#     return opt
#
#
# def set_model(opt):
#     model = SupConResNet(name=opt.model)
#     criterion = torch.nn.CrossEntropyLoss()
#
#     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
#
#     ckpt = torch.load(opt.ckpt, map_location='cpu')
#     state_dict = ckpt['model']
#
#     if torch.cuda.is_available():
#         if torch.cuda.device_count() > 1:
#             model = torch.nn.DataParallel(model)
#         else:
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k = k.replace("module.", "")
#                 new_state_dict[k] = v
#             state_dict = new_state_dict
#         model = model.cuda()
#         classifier = classifier.cuda()
#         criterion = criterion.cuda()
#         cudnn.benchmark = True
#
#         model.load_state_dict(state_dict)
#
#     return model, classifier, criterion
#
#
# def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
#     """one epoch training"""
#     model.eval()
#     classifier.train()
#
#     batch_time = AverageMeter()
#     data_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     end = time.time()
#     for idx, (images, labels) in enumerate(train_loader):
#         images = torch.cat([images[0], images[1]], dim=0)
#
#         data_time.update(time.time() - end)
#
#         images = images.cuda(non_blocking=True)
#         labels = labels.cuda(non_blocking=True)
#         bsz = labels.shape[0]
#
#         # warm-up learning rate
#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
#
#         # compute loss
#         with torch.no_grad():
#             features = model(images)
#
#         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#
#
#
#         device = (torch.device('cuda')
#                   if features.is_cuda
#                   else torch.device('cpu'))
#
#
#
#         batch_size = features.shape[0]
#
#         labels = labels.contiguous().view(-1, 1)
#         if labels.shape[0] != batch_size:
#             raise ValueError('Num of labels does not match num of features')
#         mask = torch.eq(labels, labels.T).float().to(device)
#
#
#         contrast_count = features.shape[1]
#         contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
#
#         # tile mask
#         mask = mask.repeat(2, contrast_count)
#         # mask-out self-contrast cases
#         logits_mask = torch.scatter(
#             torch.ones_like(mask),
#             1,
#             torch.arange(batch_size * 2).view(-1, 1).to(device),
#             0
#         )
#         mask = mask * logits_mask
#
#         ### 计算同一类图片增强后差值的L2范数
#         # print(mask[0,:],mask.shape)
#         # print(mask[1,:],mask.shape)
#
#         num = 0
#         mask = mask * logits_mask
#         index_mask = mask[:, :] == 1
#         sum_loss_64 = torch.Tensor([0]).cuda()
#         div = torch.Tensor([0]).cuda()
#         for i in range(len(mask)):
#             num += 1
#             sum_loss_64 += torch.sum(
#                 torch.norm((contrast_feature[i][192:] - contrast_feature[index_mask[i, :]][:, 192:]), dim=1)) / (
#                                len(contrast_feature[index_mask[i, :]]))
#
#             c_192 = F.normalize(
#                 torch.cat((contrast_feature[i:i + 1, :192], contrast_feature[index_mask[i, :]][:, :192]), dim=0), dim=1)
#             s_64 = F.normalize(
#                 torch.cat((contrast_feature[i:i + 1, 192:], contrast_feature[index_mask[i, :]][:, 192:]), dim=0).repeat(
#                     1, 3), dim=1)
#
#             div += torch.sum(torch.norm(c_192 - s_64, dim=1)) / len(torch.norm(c_192 - s_64, dim=1))
#
#         sum_loss_64 = -0.001 * sum_loss_64
#         div /= -num
#         div *= 0.1
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#         # ###
#         # f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         # features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         features, features_s_64 = torch.split(features, [192, 64], dim=2)
#         features = torch.cat(torch.unbind(features, dim=1), dim=0)
#
#         # labels = labels.contiguous().view(-1, 1)
#
#
#         labels = torch.cat((labels, labels), dim=0)
#
#         output = classifier(features.detach())
#
#
#         labels=labels.squeeze(1)
#         loss = criterion(output, labels)
#         loss=loss+div+sum_loss_64
#
#         # update metric
#         losses.update(loss.item(), bsz)
#         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#         top1.update(acc1[0], bsz)
#
#         # SGD
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # measure elapsed time
#         batch_time.update(time.time() - end)
#         end = time.time()
#
#         # print info
#         if (idx + 1) % opt.print_freq == 0:
#             print('Train: [{0}][{1}/{2}]\t'
#                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
#                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
#                    data_time=data_time, loss=losses, top1=top1))
#             sys.stdout.flush()
#
#     return losses.avg, top1.avg
#
#
# def validate(val_loader, model, classifier, criterion, opt):
#     """validation"""
#     model.eval()
#     classifier.eval()
#
#     batch_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     with torch.no_grad():
#         end = time.time()
#         for idx, (images, labels) in enumerate(val_loader):
#
#
#
#             images = images.float().cuda()
#             labels = labels.cuda()
#             bsz = labels.shape[0]
#
#             # forward
#             features=model(images)
#             features, features_64 = torch.split(features, [192, 64], dim=1)
#
#             output = classifier(features)
#
#             # output1=classifier(features_64,64)
#
#             loss = criterion(output, labels)
#
#             # update metric
#             losses.update(loss.item(), bsz)
#             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#             top1.update(acc1[0], bsz)
#
#             # measure elapsed time
#             batch_time.update(time.time() - end)
#             end = time.time()
#
#             if idx % opt.print_freq == 0:
#                 print('Test: [{0}/{1}]\t'
#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                        idx, len(val_loader), batch_time=batch_time,
#                        loss=losses, top1=top1))
#
#     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
#     return losses.avg, top1.avg
#
#
# def main():
#     best_acc = 0
#
#
#     for ep in range(1000,3250,250):
#         opt = parse_option()
#
#         # build data loader
#         train_loader, val_loader = set_loader(opt)
#
#         # build model and criterion
#         model, classifier, criterion = set_model(opt)
#
#         # build optimizer
#         optimizer = set_optimizer(opt, classifier)
#
#         # training routine
#         for epoch in range(1, opt.epochs + 1):
#             adjust_learning_rate(opt, optimizer, epoch)
#
#             # train for one epoch
#             time1 = time.time()
#             loss, acc = train(train_loader, model, classifier, criterion,
#                               optimizer, epoch, opt)
#             time2 = time.time()
#             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
#                 epoch, time2 - time1, acc))
#
#             # eval for one epoch
#             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
#             if val_acc > best_acc:
#                 best_acc = val_acc
#
#         print('best accuracy: {:.2f}'.format(best_acc))
#
#
# if __name__ == '__main__':
#     main()
#

























#
#
# from __future__ import print_function
#
# import sys
# import argparse
# import time
# import math
#
# import torch
# import torch.backends.cudnn as cudnn
#
# from main_ce import set_loader
# from util import AverageMeter
# from util import adjust_learning_rate, warmup_learning_rate, accuracy
# from util import set_optimizer
# from networks.resnet_big import SupConResNet, LinearClassifier
#
#
#
# import torch.nn.functional as F
#
# try:
#     import apex
#     from apex import amp, optimizers
# except ImportError:
#     pass
#
#
# def parse_option():
#     parser = argparse.ArgumentParser('argument for training')
#
#     parser.add_argument('--print_freq', type=int, default=10,
#                         help='print frequency')
#     parser.add_argument('--save_freq', type=int, default=50,
#                         help='save frequency')
#     parser.add_argument('--batch_size', type=int, default=256,# 256
#                         help='batch_size')
#     parser.add_argument('--num_workers', type=int, default=4,
#                         help='num of workers to use')
#     parser.add_argument('--epochs', type=int, default=100,
#                         help='number of training epochs')
#
#     # optimization
#     parser.add_argument('--learning_rate', type=float, default=0.1,
#                         help='learning rate')
#     parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
#                         help='where to decay lr, can be a list')
#     parser.add_argument('--lr_decay_rate', type=float, default=0.2,
#                         help='decay rate for learning rate')
#     parser.add_argument('--weight_decay', type=float, default=0,
#                         help='weight decay')
#     parser.add_argument('--momentum', type=float, default=0.9,
#                         help='momentum')
#
#     # model dataset
#     parser.add_argument('--model', type=str, default='resnet50')
#     parser.add_argument('--dataset', type=str, default='cifar10',
#                         choices=['cifar10', 'cifar100'], help='dataset')
#
#     # other setting
#     # parser.add_argument('--cosine', action='store_true',
#     #                     help='using cosine annealing')
#     parser.add_argument('--cosine', default='true',
#                         help='using cosine annealing')
#     parser.add_argument('--warm', action='store_true',
#                         help='warm-up for large batch training')
#
#     parser.add_argument('--ckpt', type=str, default='.\save\SupCon\cifar10_models\SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0\ckpt_epoch_800.pth',
#                         help='path to pre-trained model')
#
#     # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth',
#     #                     help='path to pre-trained model')
#
#
#     opt = parser.parse_args()
#
#     # set the path according to the environment
#     opt.data_folder = './datasets/'
#
#     iterations = opt.lr_decay_epochs.split(',')
#     opt.lr_decay_epochs = list([])
#     for it in iterations:
#         opt.lr_decay_epochs.append(int(it))
#
#     opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
#         format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
#                opt.batch_size)
#
#     if opt.cosine:
#         opt.model_name = '{}_cosine'.format(opt.model_name)
#
#     # warm-up for large-batch training,
#     if opt.warm:
#         opt.model_name = '{}_warm'.format(opt.model_name)
#         opt.warmup_from = 0.01
#         opt.warm_epochs = 10
#         if opt.cosine:
#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
#         else:
#             opt.warmup_to = opt.learning_rate
#
#     if opt.dataset == 'cifar10':
#         opt.n_cls = 10
#     elif opt.dataset == 'cifar100':
#         opt.n_cls = 100
#     else:
#         raise ValueError('dataset not supported: {}'.format(opt.dataset))
#
#     return opt
#
#
# def set_model(opt):
#     model = SupConResNet(name=opt.model)
#     criterion = torch.nn.CrossEntropyLoss()
#
#     classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)
#
#     ckpt = torch.load(opt.ckpt, map_location='cpu')
#     state_dict = ckpt['model']
#
#     if torch.cuda.is_available():
#         if torch.cuda.device_count() > 1:
#             model = torch.nn.DataParallel(model)
#         else:
#             new_state_dict = {}
#             for k, v in state_dict.items():
#                 k = k.replace("module.", "")
#                 new_state_dict[k] = v
#             state_dict = new_state_dict
#         model = model.cuda()
#         classifier = classifier.cuda()
#         criterion = criterion.cuda()
#         cudnn.benchmark = True
#
#         model.load_state_dict(state_dict)
#
#     return model, classifier, criterion
#
#
# def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
#     """one epoch training"""
#     model.eval()
#     classifier.train()
#
#     batch_time = AverageMeter()
#     data_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     end = time.time()
#     for idx, (images, labels) in enumerate(train_loader):
#         images = torch.cat([images[0], images[1]], dim=0)
#
#         data_time.update(time.time() - end)
#
#         images = images.cuda(non_blocking=True)
#         labels = labels.cuda(non_blocking=True)
#         bsz = labels.shape[0]
#
#         # warm-up learning rate
#         warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)
#
#         # compute loss
#         with torch.no_grad():
#             features = model(images)
#
#
#         # ###
#         f1, f2 = torch.split(features, [bsz, bsz], dim=0)
#         features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
#         features, features_s_64 = torch.split(features, [192, 64], dim=2)
#         features = torch.cat(torch.unbind(features, dim=1), dim=0)
#         features_s_64=torch.cat(torch.unbind(features_s_64,dim=1),dim=0)
#         labels = labels.contiguous().view(-1, 1)
#
#
#         labels = torch.cat((labels, labels), dim=0)
#
#         output = classifier(features.detach(),1)
#         output2=classifier(features_s_64.detach(),2)
#
#         output=output[:,9]+0.1*torch.sum(output2,dim=1).unsqueeze(1)
#
#         labels=labels.squeeze(1)
#         loss = criterion(output, labels)
#         # loss2=criterion(output2,labels)
#         # loss=loss-loss2
#
#         # update metric
#         losses.update(loss.item(), bsz)
#         acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#         top1.update(acc1[0], bsz)
#
#         # SGD
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         # measure elapsed time
#         batch_time.update(time.time() - end)
#         end = time.time()
#
#         # print info
#         if (idx + 1) % opt.print_freq == 0:
#             print('Train: [{0}][{1}/{2}]\t'
#                   'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                   'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
#                   'loss {loss.val:.3f} ({loss.avg:.3f})\t'
#                   'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                    epoch, idx + 1, len(train_loader), batch_time=batch_time,
#                    data_time=data_time, loss=losses, top1=top1))
#             sys.stdout.flush()
#
#     return losses.avg, top1.avg
#
#
# def validate(val_loader, model, classifier, criterion, opt):
#     """validation"""
#     model.eval()
#     classifier.eval()
#
#     batch_time = AverageMeter()
#     losses = AverageMeter()
#     top1 = AverageMeter()
#
#     with torch.no_grad():
#         end = time.time()
#         for idx, (images, labels) in enumerate(val_loader):
#
#
#
#             images = images.float().cuda()
#             labels = labels.cuda()
#             bsz = labels.shape[0]
#
#             # forward
#             features=model(images)
#             features, features_64 = torch.split(features, [192, 64], dim=1)
#
#             output = classifier(features,1)
#
#             # output1=classifier(features_64,64)
#
#             loss = criterion(output, labels)
#
#             # update metric
#             losses.update(loss.item(), bsz)
#             acc1, acc5 = accuracy(output, labels, topk=(1, 5))
#             top1.update(acc1[0], bsz)
#
#             # measure elapsed time
#             batch_time.update(time.time() - end)
#             end = time.time()
#
#             if idx % opt.print_freq == 0:
#                 print('Test: [{0}/{1}]\t'
#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
#                        idx, len(val_loader), batch_time=batch_time,
#                        loss=losses, top1=top1))
#
#     print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
#     return losses.avg, top1.avg
#
#
# def main():
#     best_acc = 0
#
#
#     for ep in range(1000,3250,250):
#         opt = parse_option()
#
#         # build data loader
#         train_loader, val_loader = set_loader(opt)
#
#         # build model and criterion
#         model, classifier, criterion = set_model(opt)
#
#         # build optimizer
#         optimizer = set_optimizer(opt, classifier)
#
#         # training routine
#         for epoch in range(1, opt.epochs + 1):
#             adjust_learning_rate(opt, optimizer, epoch)
#
#             # train for one epoch
#             time1 = time.time()
#             loss, acc = train(train_loader, model, classifier, criterion,
#                               optimizer, epoch, opt)
#             time2 = time.time()
#             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
#                 epoch, time2 - time1, acc))
#
#             # eval for one epoch
#             loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
#             if val_acc > best_acc:
#                 best_acc = val_acc
#
#         print('best accuracy: {:.2f}'.format(best_acc))
#
#
# if __name__ == '__main__':
#     main()
#
#










from __future__ import print_function

import sys
import argparse
import time
import math

import torch
import torch.backends.cudnn as cudnn

from main_ce import set_loader
from util import AverageMeter
from util import adjust_learning_rate, warmup_learning_rate, accuracy
from util import set_optimizer
from networks.resnet_big import SupConResNet, LinearClassifier



import torch.nn.functional as F

try:
    import apex
    from apex import amp, optimizers
except ImportError:
    pass


def parse_option():
    parser = argparse.ArgumentParser('argument for training')

    parser.add_argument('--print_freq', type=int, default=10,
                        help='print frequency')
    parser.add_argument('--save_freq', type=int, default=50,
                        help='save frequency')
    parser.add_argument('--batch_size', type=int, default=256,# 256
                        help='batch_size')
    parser.add_argument('--num_workers', type=int, default=32,
                        help='num of workers to use')
    parser.add_argument('--epochs', type=int, default=100,
                        help='number of training epochs')

    # optimization
    parser.add_argument('--learning_rate', type=float, default=0.1,
                        help='learning rate')
    parser.add_argument('--lr_decay_epochs', type=str, default=' 3,6,9',# 60,75,90
                        help='where to decay lr, can be a list')
    parser.add_argument('--lr_decay_rate', type=float, default=0.2,
                        help='decay rate for learning rate')
    parser.add_argument('--weight_decay', type=float, default=0,
                        help='weight decay')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='momentum')

    # model dataset
    parser.add_argument('--model', type=str, default='resnet50')
    parser.add_argument('--dataset', type=str, default='cifar10',
                        choices=['cifar10', 'cifar100'], help='dataset')

    # other setting
    # parser.add_argument('--cosine', action='store_true',
    #                     help='using cosine annealing')
    parser.add_argument('--cosine', default='true',
                        help='using cosine annealing')
    parser.add_argument('--warm', action='store_true',
                        help='warm-up for large batch training')


    parser.add_argument('--ckpt', type=str, default='/home/wangbin/Desktop/SupContrast-master(1)/save/SupCon/cifar10_models/SupCon_cifar10_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/ckpt_epoch_150.pth',
                        help='path to pre-trained model')

    # parser.add_argument('--ckpt', type=str, default='./save/SupCon/cifar100_models/SupCon_cifar100_resnet50_lr_0.05_decay_0.0001_bsz_96_temp_0.07_trial1_0/last.pth', # 940 # 950
    #                     help='path to pre-trained model')

    opt = parser.parse_args()
    # set the path according to the environment
    opt.data_folder = './datasets/'

    iterations = opt.lr_decay_epochs.split(',')
    opt.lr_decay_epochs = list([])
    for it in iterations:
        opt.lr_decay_epochs.append(int(it))

    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\
        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,
               opt.batch_size)

    if opt.cosine:
        opt.model_name = '{}_cosine'.format(opt.model_name)

    # warm-up for large-batch training,
    if opt.warm:
        opt.model_name = '{}_warm'.format(opt.model_name)
        opt.warmup_from = 0.01
        opt.warm_epochs = 10
        if opt.cosine:
            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)
            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (
                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2
        else:
            opt.warmup_to = opt.learning_rate

    if opt.dataset == 'cifar10':
        opt.n_cls = 10
    elif opt.dataset == 'cifar100':
        opt.n_cls = 100
    else:
        raise ValueError('dataset not supported: {}'.format(opt.dataset))

    return opt


def set_model(opt):
    model = SupConResNet(name=opt.model)
    criterion = torch.nn.CrossEntropyLoss()

    classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)

    ckpt = torch.load(opt.ckpt, map_location='cpu')
    state_dict = ckpt['model']

    if torch.cuda.is_available():
        if torch.cuda.device_count() > 1:
            model = torch.nn.DataParallel(model)
        else:
            new_state_dict = {}
            for k, v in state_dict.items():
                k = k.replace("module.", "")
                new_state_dict[k] = v
            state_dict = new_state_dict
        model = model.cuda()
        classifier = classifier.cuda()
        criterion = criterion.cuda()
        cudnn.benchmark = True

        model.load_state_dict(state_dict)

    return model, classifier, criterion

def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
    """one epoch training"""
    model.eval()
    classifier.train()

    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    end = time.time()
    for idx, (images, labels) in enumerate(train_loader):
        images = torch.cat([images[0], images[1]], dim=0)

        data_time.update(time.time() - end)

        images = images.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)
        bsz = labels.shape[0]

        # warm-up learning rate
        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)

        # compute loss
        with torch.no_grad():
            features,_ = model(images)

        # ###
        f1, f2 = torch.split(features, [bsz, bsz], dim=0)
        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
        features, features_s_64 = torch.split(features, [192, 64], dim=2)
        features = torch.cat(torch.unbind(features, dim=1), dim=0)

        labels = labels.contiguous().view(-1, 1)


        labels = torch.cat((labels, labels), dim=0)

        output = classifier(features.detach())


        labels=labels.squeeze(1)
        loss = criterion(output, labels)

        # update metric
        losses.update(loss.item(), bsz)
        acc1, acc5 = accuracy(output, labels, topk=(1, 5))
        top1.update(acc1[0], bsz)

        # SGD
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        # print info
        if (idx + 1) % opt.print_freq == 0:
            print('Train: [{0}][{1}/{2}]\t'
                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
                  'loss {loss.val:.3f} ({loss.avg:.3f})\t'
                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
                   epoch, idx + 1, len(train_loader), batch_time=batch_time,
                   data_time=data_time, loss=losses, top1=top1))
            sys.stdout.flush()

    return losses.avg, top1.avg


def validate(val_loader, model, classifier, criterion, opt):
    """validation"""
    model.eval()
    classifier.eval()

    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    with torch.no_grad():
        end = time.time()
        for idx, (images, labels) in enumerate(val_loader):



            images = images.float().cuda()
            labels = labels.cuda()
            bsz = labels.shape[0]

            # forward
            features,_=model(images)
            features, features_64 = torch.split(features, [192, 64], dim=1)

            output = classifier(features)

            # output1=classifier(features_64,64)

            loss = criterion(output, labels)

            # update metric
            losses.update(loss.item(), bsz)
            acc1, acc5 = accuracy(output, labels, topk=(1, 5))
            top1.update(acc1[0], bsz)

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if idx % opt.print_freq == 0:
                print('Test: [{0}/{1}]\t'
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
                       idx, len(val_loader), batch_time=batch_time,
                       loss=losses, top1=top1))

    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
    return losses.avg, top1.avg


def main():
    best_acc = 0


    for ep in range(1000,3250,250):
        opt = parse_option()

        # build data loader
        train_loader, val_loader = set_loader(opt)

        # build model and criterion
        model, classifier, criterion = set_model(opt)

        # build optimizer
        optimizer = set_optimizer(opt, classifier)

        # training routine
        for epoch in range(1, opt.epochs + 1):
            adjust_learning_rate(opt, optimizer, epoch)

            # train for one epoch
            time1 = time.time()
            loss, acc = train(train_loader, model, classifier, criterion,
                              optimizer, epoch, opt)
            time2 = time.time()
            print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
                epoch, time2 - time1, acc))

            # eval for one epoch
            loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
            if val_acc > best_acc:
                best_acc = val_acc

        print('best accuracy: {:.2f}'.format(best_acc))


if __name__ == '__main__':
    main()



























